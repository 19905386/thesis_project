{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\os.py\n",
      "c:\\Users\\Jason\\thesis_project\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.6.3.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import seaborn as sns\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "from support import *\n",
    "from features.feature_ts import genX\n",
    "from experiment.algorithms.cluster_prep import *\n",
    "from Gauss_fit_functions import extractFIT, extractToPs , gauss, straight_line\n",
    "\n",
    "from synthetic_profiles_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the households with one year worth of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping all zero rows\n"
     ]
    }
   ],
   "source": [
    "X = genX([1994,2014], drop_0 = True).reset_index()\n",
    "\n",
    "ids = pd.read_pickle(\"Ids_of_users_with_atleast_365days_of_data.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_profiles = pd.read_csv(\"Measured_Profiles_Missing_days_replaced_sorted_lenient.csv\")\n",
    "measured_profiles = measured_profiles[measured_profiles['ProfileID'].isin(ids)]\n",
    "\n",
    "measured_profiles['date'] = pd.to_datetime(measured_profiles['date'])\n",
    "# measured_profiles['date'] = measured_profiles['date'].apply(lambda x: x.date())\n",
    "# measured_profiles.set_index(['ProfileID', 'date'], inplace = True)\n",
    "ids = np.intersect1d(measured_profiles.ProfileID.unique(), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = measured_profiles#X[X['ProfileID'].isin(ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the users data into different daytypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_winter = df[df['season'] == 'winter'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "# Extract Weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "df_winter['day_names'] = df_winter.date.dt.day_name()\n",
    "df_winter['daytype'] = df_winter.day_names.where(~df_winter.day_names.isin(weekdays), 'weekday')\n",
    "df_winter.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_winter_weekdays  = df_winter[df_winter['daytype'] == 'weekday'] # Create dataframe with only weekdays\n",
    "df_winter_weekdays.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_winter_weekdays =  df_winter_weekdays[df_winter_weekdays.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_winter = df[df['season'] == 'winter'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# Extract Weekdays\n",
    "weekends = ['Sunday', 'Saturday']\n",
    "df_winter['day_names'] = df_winter.date.dt.day_name()\n",
    "df_winter['daytype'] = df_winter.day_names.where(~df_winter.day_names.isin(weekends), 'weekend')\n",
    "df_winter.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_winter_weekend  = df_winter[df_winter['daytype'] == 'weekend'] # Create dataframe with only weekdays\n",
    "df_winter_weekend.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_winter_weekend = df_winter_weekend[df_winter_weekend.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_summer = df[df['season'] == 'summer'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# Extract Weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "weekends = ['Sunday', 'Saturday']\n",
    "df_summer['day_names'] = df_summer.date.dt.day_name()\n",
    "df_summer['daytype'] = df_summer.day_names.where(~df_summer.day_names.isin(weekdays), 'weekday')\n",
    "df_summer.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_summer_weekday  = df_summer[df_summer['daytype'] == 'weekday'] # Create dataframe with only weekdays\n",
    "df_summer_weekday.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_summer_weekday = df_summer_weekday[df_summer_weekday.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_summer = df[df['season'] == 'summer'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# Extract Weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "weekends = ['Sunday', 'Saturday']\n",
    "df_summer['day_names'] = df_summer.date.dt.day_name()\n",
    "df_summer['daytype'] = df_summer.day_names.where(~df_summer.day_names.isin(weekends), 'weekend')\n",
    "df_summer.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_summer_weekends  = df_summer[df_summer['daytype'] == 'weekend'] # Create dataframe with only weekdays\n",
    "df_summer_weekends.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_summer_weekends = df_summer_weekends[df_summer_weekends.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a random sample from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 14\n",
    "\n",
    "if sample == \"all\":\n",
    "    HWeekends_df = df_winter_weekend#.groupby(['ProfileID'])#.sample(n = 25) # Have to take a sample of 30 because winter has fewer months thus fewer weekend days\n",
    "    HWeekdays_df = df_winter_weekdays#.groupby(['ProfileID'])#.sample(n = 25)\n",
    "    LWeekends_df = df_summer_weekends#.groupby(['ProfileID'])#.sample(n = 25)\n",
    "    LWeekdays_df = df_summer_weekday#.groupby(['ProfileID'])#.sample(n = 25)\n",
    "else:\n",
    "    HWeekends_df = df_winter_weekend.groupby(['ProfileID']).sample(n = sample) # Have to take a sample of 30 because winter has fewer months thus fewer weekend days\n",
    "    HWeekdays_df = df_winter_weekdays.groupby(['ProfileID']).sample(n = sample)\n",
    "    LWeekends_df = df_summer_weekends.groupby(['ProfileID']).sample(n = sample)\n",
    "    LWeekdays_df = df_summer_weekday.groupby(['ProfileID']).sample(n = sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract gauss fit features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00426e8be394dc2bf3e7c716d62c865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "dummy_df = pd.DataFrame(data, columns=cols)\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(HWeekdays_df,id)\n",
    "    \n",
    "    if check == False:\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 500:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian Fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy_High_season_weekdays.csv')\n",
    "    #     i = 0\n",
    "\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekdays_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekdays_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a25cf126f744c19461196c2e222299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "Check is FALSE\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "dummy_df = pd.DataFrame(data, columns = cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(HWeekends_df,id)\n",
    "\n",
    "    if check == False:\n",
    "        print(\"Check is FALSE\")\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 5:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'], axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy2_High_season_weekends.csv')\n",
    "    #     i = 0\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekends_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2255bfdda0146b8adc8b0e060c21629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "dummy_df = pd.DataFrame(data, columns=cols)\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(LWeekdays_df,id)\n",
    "    \n",
    "    if check == False:\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 500:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian Fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy_High_season_weekdays.csv')\n",
    "    #     i = 0\n",
    "\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekdays_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8d06d342f04885a87ef7c80d7773c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "dummy_df = pd.DataFrame(data, columns = cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(LWeekends_df,id)\n",
    "\n",
    "   \n",
    "    if check == False:\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 500:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'], axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy_Low_season_weekends.csv')\n",
    "    #     i = 0\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekends_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the amplitudes from the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33086cbc8a574e88a5e7803dc041eac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in HWeekdays_df[HWeekdays_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(HWeekdays_df[HWeekdays_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekdays_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# HWeekdays_amplitudes_sample_30 = temporary.copy()\n",
    "HWeekdays_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069fd6bc5ca9417994a6ef0de4aab759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in HWeekends_df[HWeekends_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(HWeekends_df[HWeekends_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekends_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# HWeekends_amplitudes_sample_30 = temporary.copy()\n",
    "HWeekends_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4488e007a7674db9a5748f6e6a0e3722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in LWeekdays_df[LWeekdays_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(LWeekdays_df[LWeekdays_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekdays_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# LWeekdays_amplitudes_sample_30 = temporary.copy()\n",
    "LWeekdays_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1593e7fa961446c6bd958fa9aeab9e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in LWeekends_df[LWeekends_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(LWeekends_df[LWeekends_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekends_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# LWeekends_amplitudes_sample_30 = temporary.copy()\n",
    "LWeekends_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract standard deviation from amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(my_list):\n",
    "    #calculate population standard deviation of list \n",
    "    return (sum((x-(sum(my_list) / len(my_list)))**2 for x in my_list) / len(my_list))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def determine_standard_deviation(consumption_data, amplitudes_df):\n",
    "    daily_consumption = pd.DataFrame()\n",
    "\n",
    "    daily_consumption['Daily_Consumption'] = consumption_data.set_index([\"ProfileID\"]).sum(axis = 1)\n",
    "\n",
    "    std_deviation_df = pd.DataFrame(index = amplitudes_df.index.unique())\n",
    "\n",
    "    for id in tqdm(amplitudes_df.index.unique()):\n",
    "        try:\n",
    "            std_deviation_df.loc[id,'A1_std'] = standard_deviation(amplitudes_df.loc[id]['A1'])\n",
    "            std_deviation_df.loc[id,'A2_std'] = standard_deviation(amplitudes_df.loc[id]['A2'])\n",
    "            std_deviation_df.loc[id,'mu1_std'] = standard_deviation(amplitudes_df.loc[id]['mu1'])\n",
    "            std_deviation_df.loc[id,'mu2_std'] = standard_deviation(amplitudes_df.loc[id]['mu2'])\n",
    "            std_deviation_df.loc[id,'DC_std'] = standard_deviation(daily_consumption.loc[id]['Daily_Consumption'])\n",
    "        except TypeError:\n",
    "            print('TypeError')\n",
    "            continue\n",
    "    \n",
    "    return std_deviation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73c9eb28600401c8e4fe6085cde67ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3485522489fa49d79d6d66df9a5a6da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b31f6731944ac4996da1b849198dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc07b71a7e64b828029a33ba1d50400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HWeekday_std_deviation = determine_standard_deviation(HWeekdays_df, HWeekdays_amplitudes_sample_21)\n",
    "HWeekend_std_deviation = determine_standard_deviation(HWeekends_df, HWeekends_amplitudes_sample_21)\n",
    "LWeekday_std_deviation = determine_standard_deviation(LWeekdays_df, LWeekdays_amplitudes_sample_21)\n",
    "LWeekend_std_deviation = determine_standard_deviation(LWeekends_df, LWeekends_amplitudes_sample_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWeekdays_features = pd.read_csv('FitFeatures_HSeason_weekdays_sample_30.csv', index_col='ProfileID')\n",
    "# HWeekends_features = pd.read_csv('FitFeatures_HSeason_weekends_sample_30.csv', index_col='ProfileID')\n",
    "# LWeekdays_features = pd.read_csv('FitFeatures_LSeason_weekdays_sample_30.csv', index_col='ProfileID')\n",
    "# LWeekends_features = pd.read_csv('FitFeatures_LSeason_weekends_sample_30.csv', index_col='ProfileID')\n",
    "\n",
    "HWeekdays_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekdays_sample_' + str(sample) + '.csv', index_col='ProfileID')\n",
    "HWeekends_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekends_sample_' + str(sample) + '.csv', index_col='ProfileID')\n",
    "LWeekdays_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekdays_sample_' + str(sample) + '.csv', index_col='ProfileID')\n",
    "LWeekends_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekends_sample_' + str(sample) + '.csv', index_col='ProfileID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "HWeekdays_combined = pd.merge(HWeekdays_features,HWeekday_std_deviation, left_index = True, right_index = True)\n",
    "HWeekends_combined = pd.merge(HWeekends_features,HWeekend_std_deviation, left_index = True, right_index = True)\n",
    "LWeekdays_combined = pd.merge(LWeekdays_features,LWeekday_std_deviation, left_index = True, right_index = True)\n",
    "LWeekends_combined = pd.merge(LWeekends_features,LWeekend_std_deviation, left_index = True, right_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_offset</th>\n",
       "      <th>sigma1</th>\n",
       "      <th>sigma2</th>\n",
       "      <th>mu1</th>\n",
       "      <th>A1</th>\n",
       "      <th>sigma3</th>\n",
       "      <th>sigma4</th>\n",
       "      <th>mu2</th>\n",
       "      <th>A2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>6.373399</td>\n",
       "      <td>1.886792</td>\n",
       "      <td>1.860465</td>\n",
       "      <td>9</td>\n",
       "      <td>12.932750</td>\n",
       "      <td>2.580645</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>19</td>\n",
       "      <td>10.916768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1.885179</td>\n",
       "      <td>1.165049</td>\n",
       "      <td>1.481481</td>\n",
       "      <td>8</td>\n",
       "      <td>18.366190</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.621622</td>\n",
       "      <td>13</td>\n",
       "      <td>13.204821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>8.046440</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>3.018868</td>\n",
       "      <td>8</td>\n",
       "      <td>13.136643</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>1.739130</td>\n",
       "      <td>20</td>\n",
       "      <td>10.612607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>5.477339</td>\n",
       "      <td>0.900901</td>\n",
       "      <td>3.809524</td>\n",
       "      <td>9</td>\n",
       "      <td>4.249250</td>\n",
       "      <td>2.380952</td>\n",
       "      <td>1.470588</td>\n",
       "      <td>19</td>\n",
       "      <td>11.935976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>5.341839</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>8</td>\n",
       "      <td>2.857369</td>\n",
       "      <td>1.219512</td>\n",
       "      <td>2.758621</td>\n",
       "      <td>20</td>\n",
       "      <td>8.492167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>2.314571</td>\n",
       "      <td>1.075269</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>9.078935</td>\n",
       "      <td>1.481481</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>19</td>\n",
       "      <td>10.250577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>7.052970</td>\n",
       "      <td>1.621622</td>\n",
       "      <td>2.380952</td>\n",
       "      <td>8</td>\n",
       "      <td>12.341917</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.127660</td>\n",
       "      <td>19</td>\n",
       "      <td>14.998143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4566</th>\n",
       "      <td>4.679387</td>\n",
       "      <td>1.346154</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>9</td>\n",
       "      <td>8.429744</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>20</td>\n",
       "      <td>11.776976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>5.994935</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>10</td>\n",
       "      <td>8.855256</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>3.030303</td>\n",
       "      <td>19</td>\n",
       "      <td>19.669822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4608</th>\n",
       "      <td>2.466173</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>5</td>\n",
       "      <td>1.188149</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>19</td>\n",
       "      <td>12.358720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4610</th>\n",
       "      <td>4.786536</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>5</td>\n",
       "      <td>0.783756</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.463415</td>\n",
       "      <td>13</td>\n",
       "      <td>12.930077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>3.677792</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>7</td>\n",
       "      <td>12.570428</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>17</td>\n",
       "      <td>12.352804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001138</th>\n",
       "      <td>2.044774</td>\n",
       "      <td>1.917808</td>\n",
       "      <td>1.481481</td>\n",
       "      <td>9</td>\n",
       "      <td>13.127667</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>18</td>\n",
       "      <td>12.272738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001148</th>\n",
       "      <td>3.323173</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>2.962963</td>\n",
       "      <td>8</td>\n",
       "      <td>12.740042</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>21</td>\n",
       "      <td>15.352006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019975</th>\n",
       "      <td>7.927381</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.739130</td>\n",
       "      <td>8</td>\n",
       "      <td>14.119048</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>3.030303</td>\n",
       "      <td>17</td>\n",
       "      <td>17.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12020859</th>\n",
       "      <td>5.676191</td>\n",
       "      <td>1.269841</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>8</td>\n",
       "      <td>8.907143</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>19</td>\n",
       "      <td>14.226190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12021739</th>\n",
       "      <td>3.978571</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>8</td>\n",
       "      <td>5.913095</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>1.449275</td>\n",
       "      <td>19</td>\n",
       "      <td>8.307143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12021804</th>\n",
       "      <td>7.661905</td>\n",
       "      <td>0.854701</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>7</td>\n",
       "      <td>7.288095</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>1.967213</td>\n",
       "      <td>18</td>\n",
       "      <td>17.176190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12022623</th>\n",
       "      <td>2.442857</td>\n",
       "      <td>1.492537</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>15.365476</td>\n",
       "      <td>2.564103</td>\n",
       "      <td>2.608696</td>\n",
       "      <td>18</td>\n",
       "      <td>12.898809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12022948</th>\n",
       "      <td>0.796429</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>18.907143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12023147</th>\n",
       "      <td>1.380952</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>2</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5.185185</td>\n",
       "      <td>17</td>\n",
       "      <td>18.702381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           H_offset    sigma1    sigma2  mu1         A1    sigma3    sigma4  \\\n",
       "ProfileID                                                                     \n",
       "695        6.373399  1.886792  1.860465    9  12.932750  2.580645  1.923077   \n",
       "710        1.885179  1.165049  1.481481    8  18.366190  2.000000  1.621622   \n",
       "4519       8.046440  0.581395  3.018868    8  13.136643  1.714286  1.739130   \n",
       "4529       5.477339  0.900901  3.809524    9   4.249250  2.380952  1.470588   \n",
       "4547       5.341839  0.751880  0.555556    8   2.857369  1.219512  2.758621   \n",
       "4562       2.314571  1.075269  5.000000   10   9.078935  1.481481  2.222222   \n",
       "4564       7.052970  1.621622  2.380952    8  12.341917  2.272727  2.127660   \n",
       "4566       4.679387  1.346154  2.777778    9   8.429744  1.428571  1.818182   \n",
       "4596       5.994935  1.538462  1.666667   10   8.855256  0.816327  3.030303   \n",
       "4608       2.466173  0.006025  0.754717    5   1.188149  1.111111  2.142857   \n",
       "4610       4.786536  2.105263  0.377358    5   0.783756  3.333333  1.463415   \n",
       "5404       3.677792  0.888889  2.702703    7  12.570428  0.769231  3.181818   \n",
       "1001138    2.044774  1.917808  1.481481    9  13.127667  2.857143  2.142857   \n",
       "1001148    3.323173  0.941176  2.962963    8  12.740042  3.333333  2.222222   \n",
       "12019975   7.927381  1.369863  1.739130    8  14.119048  4.347826  3.030303   \n",
       "12020859   5.676191  1.269841  1.250000    8   8.907143  1.538462  1.388889   \n",
       "12021739   3.978571  1.333333  1.538462    8   5.913095  3.076923  1.449275   \n",
       "12021804   7.661905  0.854701  3.333333    7   7.288095  1.538462  1.967213   \n",
       "12022623   2.442857  1.492537  2.500000    8  15.365476  2.564103  2.608696   \n",
       "12022948   0.796429  2.857143  1.600000    1  22.750000  2.857143  5.000000   \n",
       "12023147   1.380952  0.004016  0.377358    2   0.567857  3.333333  5.185185   \n",
       "\n",
       "           mu2         A2  \n",
       "ProfileID                  \n",
       "695         19  10.916768  \n",
       "710         13  13.204821  \n",
       "4519        20  10.612607  \n",
       "4529        19  11.935976  \n",
       "4547        20   8.492167  \n",
       "4562        19  10.250577  \n",
       "4564        19  14.998143  \n",
       "4566        20  11.776976  \n",
       "4596        19  19.669822  \n",
       "4608        19  12.358720  \n",
       "4610        13  12.930077  \n",
       "5404        17  12.352804  \n",
       "1001138     18  12.272738  \n",
       "1001148     21  15.352006  \n",
       "12019975    17  17.516667  \n",
       "12020859    19  14.226190  \n",
       "12021739    19   8.307143  \n",
       "12021804    18  17.176190  \n",
       "12022623    18  12.898809  \n",
       "12022948    21  18.907143  \n",
       "12023147    17  18.702381  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HWeekends_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Distributions to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiles_ids = combined_df.index.unique().values\n",
    "\n",
    "def determine_distributions(amplitudes_df):\n",
    "\n",
    "  cols = pd.MultiIndex.from_tuples([#(\"ProfileID\",''),\n",
    "                                  ('A1', 'Distribution'),\n",
    "                                  (\"A1\", \"chi_square\"), \n",
    "                                    (\"A1\", \"params\"), \n",
    "                                    (\"A2\", \"Distribution\"),\n",
    "                                    (\"A2\", \"chi_square\"),\n",
    "                                    (\"A2\", \"params\") \n",
    "                                    #, ('t1', 'Distribution'),\n",
    "                                    # (\"t1\", \"chi_square\"), \n",
    "                                    # (\"t1\", \"params\"), \n",
    "                                    # (\"t2\", \"Distribution\"),\n",
    "                                    # (\"t2\", \"chi_square\"),\n",
    "                                    # (\"t2\", \"params\"),\n",
    "                                  ])\n",
    "  distributions_df = pd.DataFrame(index = ids,columns = cols)\n",
    "  results = []\n",
    "  for id in tqdm(ids):\n",
    "    # Extract the best distribution fitted\n",
    "    try:\n",
    "      results1 = fit_distribution(amplitudes_df.loc[id],'A1',0.99,0.01)\n",
    "      results2 = fit_distribution(amplitudes_df.loc[id],'A2',0.99,0.01)\n",
    "      # results3 = fit_distribution(temporary.loc[id],'mu1',0.99,0.01)\n",
    "      # results4 = fit_distribution(temporary.loc[id],'mu2',0.99,0.01)\n",
    "\n",
    "      results = [results1.values[0],results1.values[1],results1.values[2], results2.values[0],results2.values[1],results2.values[2]]\n",
    "                # ,results3.values[0],results3.values[1],results3.values[2], results4.values[0],results4.values[1],results4.values[2]]\n",
    "\n",
    "      distributions_df.loc[id] = results\n",
    "    except Exception:\n",
    "      continue\n",
    "\n",
    "  return distributions_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdcb5cfcd274cb7be39f43107ea68de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f5de65adbb442b9d5a62d45d0851e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc18673a1224d599341c796cd786ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c06d9ebc20547f3b9bad7e6657f78b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HWeekdays_distributions = determine_distributions(HWeekdays_amplitudes_sample_21)\n",
    "HWeekends_distributions = determine_distributions(HWeekends_amplitudes_sample_21)\n",
    "LWeekdays_distributions = determine_distributions(LWeekdays_amplitudes_sample_21)\n",
    "LWeekends_distributions = determine_distributions(LWeekends_amplitudes_sample_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the distributions for the sample of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWeekdays_distributions.to_csv(\"HWeekdays_distributions_sample_30.csv\")\n",
    "# HWeekends_distributions.to_csv(\"HWeekends_distributions_sample_30.csv\") \n",
    "# LWeekdays_distributions.to_csv(\"LWeekdays_distributions_sample_30.csv\")\n",
    "# LWeekends_distributions.to_csv(\"LWeekends_distributions_sample_30.csv\")\n",
    "\n",
    "HWeekdays_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekdays_distributions_sample_\" + str(sample) + \".csv\")\n",
    "HWeekends_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekends_distributions_sample_\" + str(sample) + \".csv\") \n",
    "LWeekdays_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekdays_distributions_sample_\" + str(sample) + \".csv\")\n",
    "LWeekends_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekends_distributions_sample_\" + str(sample) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the synthetic profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_peaks(distributions_df, combined_df):\n",
    "    level_0 = distributions_df.columns.get_level_values(0).unique()\n",
    "    inv_data_df = pd.DataFrame()\n",
    "    temp_df_inv = pd.DataFrame()\n",
    "    for houseID in tqdm(distributions_df.index):\n",
    "        try:\n",
    "            for column in level_0:\n",
    "\n",
    "                distributions = distributions_df[column].loc[houseID]['Distribution']\n",
    "                parameters = distributions_df[column].loc[houseID]['params']\n",
    "\n",
    "                # parameters = eval(parameters)\n",
    "                loc = combined_df.loc[houseID][column] + combined_df.loc[houseID]['H_offset']\n",
    "                scale = combined_df.loc[houseID][column + '_std']  \n",
    "                # loc = parameters[-2]\n",
    "                # scale = parameters[-1]\n",
    "                size = 300\n",
    "\n",
    "                if distributions == 'invgauss':\n",
    "                    # print('invgauss')\n",
    "                    data_points = invgauss.rvs(parameters[0],loc = loc,scale = scale,size = size)\n",
    "                elif distributions == 'weibull_min':\n",
    "                    # print('weibull_min')\n",
    "                    data_points = weibull_min.rvs(parameters[0], loc = loc,scale = scale, size = size)       \n",
    "                elif distributions == 'lognorm':\n",
    "                    # print('lognorm')\n",
    "                    data_points = lognorm.rvs(parameters[0], loc = loc,scale = scale, size = size)            \n",
    "                elif distributions == 'expon':\n",
    "                    # print('expon')\n",
    "                    data_points = expon.rvs(loc = loc,scale = scale, size = size)\n",
    "                elif distributions == 'gamma':\n",
    "                    # print('gamma')\n",
    "                    data_points = gamma.rvs(parameters[0], loc = loc,scale = scale, size = size)            \n",
    "                elif distributions == 'halflogistic':\n",
    "                    # print('halflogistic')\n",
    "                    data_points = halflogistic.rvs(loc=loc, scale = scale,size=size)\n",
    "                \n",
    "                \n",
    "\n",
    "                # print(column)\n",
    "                # inverse_data_points = inverse_StandardScalar(data_points,temporary.loc[id],column,0.99,0.01)\n",
    "                temp_df_inv['ProfileID'] = houseID\n",
    "                temp_df_inv[column] = data_points#inverse_data_points\n",
    "                \n",
    "            inv_data_df = inv_data_df.append(temp_df_inv)\n",
    "            temp_df_inv = pd.DataFrame()\n",
    "                # if column == 'A1':\n",
    "                #     data_pointA1 = data_points#inverse_data_points\n",
    "                # elif column == 'A2':\n",
    "                #     data_pointsA2 = data_points#inverse_data_points\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: {houseID}\")\n",
    "            continue\n",
    "\n",
    "    inv_data_df = inv_data_df.dropna()\n",
    "    inv_data_df.set_index(['ProfileID'], inplace = True)\n",
    "\n",
    "    return inv_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d5a5782ad1417f855a2a4811a34c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af8a4da673b4bc9a5c228a0b7877621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 4525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dfcee414f548f8958a3484642997f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b41f04a9434975b7a8690296be07a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 4610\n",
      "KeyError: 12022948\n"
     ]
    }
   ],
   "source": [
    "HWeekday_synthetic_peaks = generate_synthetic_peaks(HWeekdays_distributions, HWeekdays_combined)\n",
    "HWeekend_synthetic_peaks = generate_synthetic_peaks(HWeekends_distributions, HWeekends_combined)\n",
    "LWeekday_synthetic_peaks = generate_synthetic_peaks(LWeekdays_distributions, LWeekdays_combined)\n",
    "LWeekend_synthetic_peaks = generate_synthetic_peaks(LWeekends_distributions, LWeekends_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers from peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Outlier_Indices(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.95)\n",
    "    IQR = Q3 - Q1\n",
    "    trueList = ~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR)))\n",
    "    return trueList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "HWeekday_synthetic_peaks = HWeekday_synthetic_peaks[Remove_Outlier_Indices(HWeekday_synthetic_peaks['A1'])]\n",
    "HWeekday_synthetic_peaks = HWeekday_synthetic_peaks[Remove_Outlier_Indices(HWeekday_synthetic_peaks['A2'])]\n",
    "\n",
    "HWeekend_synthetic_peaks = HWeekend_synthetic_peaks[Remove_Outlier_Indices(HWeekend_synthetic_peaks['A1'])]\n",
    "HWeekend_synthetic_peaks = HWeekend_synthetic_peaks[Remove_Outlier_Indices(HWeekend_synthetic_peaks['A2'])]\n",
    "\n",
    "LWeekday_synthetic_peaks = LWeekday_synthetic_peaks[Remove_Outlier_Indices(LWeekday_synthetic_peaks['A1'])]\n",
    "LWeekday_synthetic_peaks = LWeekday_synthetic_peaks[Remove_Outlier_Indices(LWeekday_synthetic_peaks['A2'])]\n",
    "\n",
    "LWeekend_synthetic_peaks = LWeekend_synthetic_peaks[Remove_Outlier_Indices(LWeekend_synthetic_peaks['A1'])]\n",
    "LWeekend_synthetic_peaks = LWeekend_synthetic_peaks[Remove_Outlier_Indices(LWeekend_synthetic_peaks['A2'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the synthetic profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_profiles(distributions_df, features_df, synthetic_peaks): \n",
    "    synthetic_df = pd.DataFrame()\n",
    "    for id in tqdm(distributions_df.index.unique()):\n",
    "        try:\n",
    "            houseID = id\n",
    "\n",
    "            H_offset = features_df.loc[houseID]['H_offset']\n",
    "            # H_offset = 0.0\n",
    "            mu1 = features_df.loc[houseID]['mu1']\n",
    "            mu2 = features_df.loc[houseID]['mu2']\n",
    "\n",
    "            sigma1 = features_df.loc[houseID]['sigma1']\n",
    "            sigma2 = features_df.loc[houseID]['sigma2']\n",
    "            sigma3 = features_df.loc[houseID]['sigma3']\n",
    "            sigma4 = features_df.loc[houseID]['sigma4']\n",
    "\n",
    "            A1 = synthetic_peaks.loc[houseID]['A1']\n",
    "            A2 = synthetic_peaks.loc[houseID]['A2']\n",
    "            A1 = pd.DataFrame(A1)\n",
    "            A2 = pd.DataFrame(A2)\n",
    "            A1 = A1 - H_offset\n",
    "            A2 = A2 - H_offset\n",
    "\n",
    "            A1.reset_index(inplace=True)\n",
    "            A2.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "            synth = generate_synth_profiles2(houseID,A1,A2,mu1,mu2,H_offset,sigma1,sigma2, sigma3,sigma4)\n",
    "\n",
    "            # temp = pd.DataFrame(synth)\n",
    "            # temp = temp.T\n",
    "            synthetic_df = synthetic_df.append(synth)\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: {id}\")\n",
    "            continue\n",
    "        \n",
    "    return synthetic_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c598112a15402b9a3e87a85ff77a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e89e2b0a39481bb51b71fe5572989c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 695\n",
      "KeyError: 4525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403277e14e214fdb8f48780bf59be58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 12021804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66823b89d5af488fb1cddbe2c77e1af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 4610\n",
      "KeyError: 12022948\n"
     ]
    }
   ],
   "source": [
    "HWeekday_synthetic_profiles = create_synthetic_profiles(HWeekdays_distributions, HWeekdays_features, HWeekday_synthetic_peaks)\n",
    "HWeekend_synthetic_profiles = create_synthetic_profiles(HWeekends_distributions, HWeekends_features, HWeekend_synthetic_peaks)\n",
    "LWeekday_synthetic_profiles = create_synthetic_profiles(LWeekdays_distributions, LWeekdays_features, LWeekday_synthetic_peaks)\n",
    "LWeekend_synthetic_profiles = create_synthetic_profiles(LWeekends_distributions, LWeekends_features, LWeekend_synthetic_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the synthetic sample profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWeekday_synthetic_profiles.to_csv(\"HWeekday_synthetic_profiles_sample_30.csv\")\n",
    "# HWeekend_synthetic_profiles.to_csv(\"HWeekend_synthetic_profiles_sample_30.csv\")\n",
    "# LWeekday_synthetic_profiles.to_csv(\"LWeekday_synthetic_profiles_sample_30.csv\")\n",
    "# LWeekend_synthetic_profiles.to_csv(\"LWeekend_synthetic_profiles_sample_30.csv\")\n",
    "\n",
    "HWeekday_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekday_synthetic_profiles_sample_\" + str(sample) + \".csv\")\n",
    "HWeekend_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekend_synthetic_profiles_sample_\" + str(sample) + \".csv\")\n",
    "LWeekday_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekday_synthetic_profiles_sample_\" + str(sample) + \".csv\")\n",
    "LWeekend_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekend_synthetic_profiles_sample_\" + str(sample) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation with sample n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_profiles = pd.read_csv(\"Measured_Profiles_Missing_days_replaced_sorted_lenient.csv\")\n",
    "measured_profiles = measured_profiles[measured_profiles['ProfileID'].isin(ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileID</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">695</th>\n",
       "      <th>1996-01-01</th>\n",
       "      <td>0.985750</td>\n",
       "      <td>4.109417</td>\n",
       "      <td>1.074250</td>\n",
       "      <td>1.419250</td>\n",
       "      <td>3.643583</td>\n",
       "      <td>10.296583</td>\n",
       "      <td>10.082417</td>\n",
       "      <td>9.147667</td>\n",
       "      <td>10.835666</td>\n",
       "      <td>2.116750</td>\n",
       "      <td>...</td>\n",
       "      <td>11.612750</td>\n",
       "      <td>8.668333</td>\n",
       "      <td>3.388667</td>\n",
       "      <td>7.225583</td>\n",
       "      <td>8.197500</td>\n",
       "      <td>5.243833</td>\n",
       "      <td>4.677750</td>\n",
       "      <td>2.391833</td>\n",
       "      <td>2.068917</td>\n",
       "      <td>1.648250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-02</th>\n",
       "      <td>1.907667</td>\n",
       "      <td>1.904167</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>0.745917</td>\n",
       "      <td>2.515500</td>\n",
       "      <td>8.386333</td>\n",
       "      <td>9.517416</td>\n",
       "      <td>14.535250</td>\n",
       "      <td>15.029833</td>\n",
       "      <td>7.886000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.141500</td>\n",
       "      <td>14.060333</td>\n",
       "      <td>14.050333</td>\n",
       "      <td>5.163000</td>\n",
       "      <td>14.639417</td>\n",
       "      <td>11.439667</td>\n",
       "      <td>14.850416</td>\n",
       "      <td>14.425333</td>\n",
       "      <td>7.889250</td>\n",
       "      <td>1.879750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-03</th>\n",
       "      <td>1.217000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>1.981833</td>\n",
       "      <td>3.594417</td>\n",
       "      <td>1.455667</td>\n",
       "      <td>10.883000</td>\n",
       "      <td>15.531333</td>\n",
       "      <td>14.280333</td>\n",
       "      <td>13.538000</td>\n",
       "      <td>4.530500</td>\n",
       "      <td>...</td>\n",
       "      <td>10.132667</td>\n",
       "      <td>7.960417</td>\n",
       "      <td>5.208333</td>\n",
       "      <td>11.852750</td>\n",
       "      <td>13.416583</td>\n",
       "      <td>5.832083</td>\n",
       "      <td>9.138500</td>\n",
       "      <td>11.399667</td>\n",
       "      <td>2.311500</td>\n",
       "      <td>2.070083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>1.403833</td>\n",
       "      <td>1.487583</td>\n",
       "      <td>1.363167</td>\n",
       "      <td>2.727000</td>\n",
       "      <td>2.300917</td>\n",
       "      <td>10.251583</td>\n",
       "      <td>9.054167</td>\n",
       "      <td>6.212333</td>\n",
       "      <td>3.919417</td>\n",
       "      <td>4.546083</td>\n",
       "      <td>...</td>\n",
       "      <td>9.270917</td>\n",
       "      <td>10.222667</td>\n",
       "      <td>14.477667</td>\n",
       "      <td>11.548417</td>\n",
       "      <td>10.390000</td>\n",
       "      <td>10.334333</td>\n",
       "      <td>9.304167</td>\n",
       "      <td>11.652083</td>\n",
       "      <td>2.681083</td>\n",
       "      <td>2.354083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>1.239750</td>\n",
       "      <td>2.198667</td>\n",
       "      <td>1.280500</td>\n",
       "      <td>2.503500</td>\n",
       "      <td>2.171000</td>\n",
       "      <td>10.705250</td>\n",
       "      <td>10.645000</td>\n",
       "      <td>7.862417</td>\n",
       "      <td>6.695750</td>\n",
       "      <td>5.926417</td>\n",
       "      <td>...</td>\n",
       "      <td>11.801167</td>\n",
       "      <td>20.552417</td>\n",
       "      <td>9.668500</td>\n",
       "      <td>4.797333</td>\n",
       "      <td>5.144333</td>\n",
       "      <td>6.551500</td>\n",
       "      <td>5.032500</td>\n",
       "      <td>2.160417</td>\n",
       "      <td>2.428667</td>\n",
       "      <td>2.770167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">12023147</th>\n",
       "      <th>2012-12-27</th>\n",
       "      <td>3.516667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.566667</td>\n",
       "      <td>1.616667</td>\n",
       "      <td>4.233333</td>\n",
       "      <td>4.816667</td>\n",
       "      <td>9.433333</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>6.850000</td>\n",
       "      <td>8.866667</td>\n",
       "      <td>8.550000</td>\n",
       "      <td>15.316667</td>\n",
       "      <td>6.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-28</th>\n",
       "      <td>3.033333</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.483333</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>4.466667</td>\n",
       "      <td>8.133333</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>6.766667</td>\n",
       "      <td>...</td>\n",
       "      <td>12.583333</td>\n",
       "      <td>8.133333</td>\n",
       "      <td>6.616667</td>\n",
       "      <td>5.133333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.983333</td>\n",
       "      <td>7.016667</td>\n",
       "      <td>14.116667</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>4.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-29</th>\n",
       "      <td>4.016667</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>3.116667</td>\n",
       "      <td>6.183333</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>11.833333</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>10.516667</td>\n",
       "      <td>10.866667</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>8.483333</td>\n",
       "      <td>9.183333</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-30</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.683333</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>7.550000</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>10.033333</td>\n",
       "      <td>5.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-31</th>\n",
       "      <td>4.350000</td>\n",
       "      <td>3.366667</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>3.283333</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>4.533333</td>\n",
       "      <td>6.466667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>6.683333</td>\n",
       "      <td>8.266667</td>\n",
       "      <td>8.683333</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>5.433333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8049 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1         2         3         4  \\\n",
       "ProfileID date                                                           \n",
       "695       1996-01-01  0.985750  4.109417  1.074250  1.419250  3.643583   \n",
       "          1996-01-02  1.907667  1.904167  0.431250  0.745917  2.515500   \n",
       "          1996-01-03  1.217000  1.090000  1.981833  3.594417  1.455667   \n",
       "          1996-01-04  1.403833  1.487583  1.363167  2.727000  2.300917   \n",
       "          1996-01-05  1.239750  2.198667  1.280500  2.503500  2.171000   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "12023147  2012-12-27  3.516667  1.666667  3.666667  1.583333  1.566667   \n",
       "          2012-12-28  3.033333  1.600000  1.483333  1.533333  1.533333   \n",
       "          2012-12-29  4.016667  1.850000  1.833333  1.650000  1.700000   \n",
       "          2012-12-30  2.250000  1.700000  0.483333  0.466667  0.516667   \n",
       "          2012-12-31  4.350000  3.366667  2.750000  2.000000  3.066667   \n",
       "\n",
       "                              5          6          7          8         9  \\\n",
       "ProfileID date                                                               \n",
       "695       1996-01-01  10.296583  10.082417   9.147667  10.835666  2.116750   \n",
       "          1996-01-02   8.386333   9.517416  14.535250  15.029833  7.886000   \n",
       "          1996-01-03  10.883000  15.531333  14.280333  13.538000  4.530500   \n",
       "          1996-01-04  10.251583   9.054167   6.212333   3.919417  4.546083   \n",
       "          1996-01-05  10.705250  10.645000   7.862417   6.695750  5.926417   \n",
       "...                         ...        ...        ...        ...       ...   \n",
       "12023147  2012-12-27   1.616667   4.233333   4.816667   9.433333  7.500000   \n",
       "          2012-12-28   1.533333   4.466667   8.133333   3.833333  6.766667   \n",
       "          2012-12-29   2.266667   1.650000   3.116667   6.183333  5.300000   \n",
       "          2012-12-30   2.000000   2.550000   1.733333   8.700000  8.050000   \n",
       "          2012-12-31   3.066667   3.283333   7.500000   4.533333  6.466667   \n",
       "\n",
       "                      ...         14         15         16         17  \\\n",
       "ProfileID date        ...                                               \n",
       "695       1996-01-01  ...  11.612750   8.668333   3.388667   7.225583   \n",
       "          1996-01-02  ...  11.141500  14.060333  14.050333   5.163000   \n",
       "          1996-01-03  ...  10.132667   7.960417   5.208333  11.852750   \n",
       "          1996-01-04  ...   9.270917  10.222667  14.477667  11.548417   \n",
       "          1996-01-05  ...  11.801167  20.552417   9.668500   4.797333   \n",
       "...                   ...        ...        ...        ...        ...   \n",
       "12023147  2012-12-27  ...   3.466667   4.400000   5.850000  10.133333   \n",
       "          2012-12-28  ...  12.583333   8.133333   6.616667   5.133333   \n",
       "          2012-12-29  ...   5.950000  12.850000  11.833333   4.416667   \n",
       "          2012-12-30  ...   7.683333  10.133333  10.250000   4.916667   \n",
       "          2012-12-31  ...   5.933333   9.016667   4.683333   4.683333   \n",
       "\n",
       "                             18         19         20         21         22  \\\n",
       "ProfileID date                                                                \n",
       "695       1996-01-01   8.197500   5.243833   4.677750   2.391833   2.068917   \n",
       "          1996-01-02  14.639417  11.439667  14.850416  14.425333   7.889250   \n",
       "          1996-01-03  13.416583   5.832083   9.138500  11.399667   2.311500   \n",
       "          1996-01-04  10.390000  10.334333   9.304167  11.652083   2.681083   \n",
       "          1996-01-05   5.144333   6.551500   5.032500   2.160417   2.428667   \n",
       "...                         ...        ...        ...        ...        ...   \n",
       "12023147  2012-12-27   7.733333   6.850000   8.866667   8.550000  15.316667   \n",
       "          2012-12-28   5.000000   9.983333   7.016667  14.116667   7.800000   \n",
       "          2012-12-29  10.516667  10.866667   5.250000   8.483333   9.183333   \n",
       "          2012-12-30   8.833333   7.550000   3.933333   9.450000  10.033333   \n",
       "          2012-12-31   6.683333   8.266667   8.683333   8.833333   8.600000   \n",
       "\n",
       "                            23  \n",
       "ProfileID date                  \n",
       "695       1996-01-01  1.648250  \n",
       "          1996-01-02  1.879750  \n",
       "          1996-01-03  2.070083  \n",
       "          1996-01-04  2.354083  \n",
       "          1996-01-05  2.770167  \n",
       "...                        ...  \n",
       "12023147  2012-12-27  6.450000  \n",
       "          2012-12-28  4.283333  \n",
       "          2012-12-29  5.666667  \n",
       "          2012-12-30  5.950000  \n",
       "          2012-12-31  5.433333  \n",
       "\n",
       "[8049 rows x 24 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measured_profiles['date'] = pd.to_datetime(measured_profiles['date'])\n",
    "measured_profiles['date'] = measured_profiles['date'].apply(lambda x: x.date())\n",
    "measured_profiles.set_index(['ProfileID', 'date'], inplace = True)\n",
    "measured_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one year of synthetic profiles for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0427a62589c549fc839369348bc6ffa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 695\n",
      "KeyError: 4525\n",
      "KeyError: 4610\n",
      "KeyError: 12021804\n",
      "KeyError: 12022948\n"
     ]
    }
   ],
   "source": [
    "temp_synth = pd.DataFrame(index = measured_profiles.index)\n",
    "temp_synth.reset_index(['date'], inplace = True)\n",
    "\n",
    "temp_synth['month'] = temp_synth.date.dt.month\n",
    "temp_synth['month'] = temp_synth['month'].apply(lambda x: 'HIGH' if x in [6, 7, 8] else 'LOW') \n",
    "temp_synth['day_names'] = temp_synth.date.dt.day_name()\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "temp_synth['day_names'] =  temp_synth['day_names'].apply(lambda x: 'WEEKDAY' if x in weekdays else 'WEEKEND') \n",
    "\n",
    "df_frames = []\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    try:\n",
    "        try:\n",
    "            temp1 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        LWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp1 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        LWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]), replace = True)], axis = 1)\n",
    "        try:                \n",
    "            temp2 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        LWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp2 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        LWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]), replace = True)], axis = 1)\n",
    "\n",
    "        try:\n",
    "            temp3 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        HWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp3 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        HWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]), replace = True)], axis = 1)\n",
    "                    \n",
    "        try:\n",
    "            temp4 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        HWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp4 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        HWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]), replace = True)], axis = 1)\n",
    "\n",
    "            \n",
    "        frames = [temp1, temp2, temp3, temp4]\n",
    "        result = pd.concat(frames)\n",
    "\n",
    "        df_frames.insert(len(df_frames)+1,result)\n",
    "    except KeyError:\n",
    "        print(f\"KeyError: {id}\")\n",
    "\n",
    "result2 = pd.concat(df_frames)\n",
    "\n",
    "result2.reset_index(inplace = True)\n",
    "result2.set_index(['ProfileID', 'date'], inplace = True)\n",
    "result2 = result2.sort_index()\n",
    "# result2 = result2.drop(['24'], axis = 1)\n",
    "result2.drop(['month', 'day_names'], axis = 1, inplace = True)\n",
    "\n",
    "synthetic_profiles = result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_profiles.to_csv('CSV_Files/' + str(sample) + '/Synthetic_Profiles_sample_' + str(sample) + '.csv')\n",
    "# synthetic_profiles = pd.read_csv('Synthetic_Profiles_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75273f6ed8af91899ebc591bf6ae2fd0716c5db2515d7097a000123632f4e53a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
