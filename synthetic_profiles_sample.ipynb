{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\os.py\n",
      "c:\\Users\\Jason\\thesis_project\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.6.3.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import seaborn as sns\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "from support import *\n",
    "from features.feature_ts import genX\n",
    "from experiment.algorithms.cluster_prep import *\n",
    "from Gauss_fit_functions import extractFIT, extractToPs , gauss, straight_line\n",
    "\n",
    "from synthetic_profiles_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the households with one year worth of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping all zero rows\n"
     ]
    }
   ],
   "source": [
    "X = genX([1994,2014], drop_0 = True).reset_index()\n",
    "\n",
    "ids = pd.read_pickle(\"Ids_of_users_with_atleast_365days_of_data.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_profiles = pd.read_csv(\"Measured_Profiles_Missing_days_replaced_sorted_lenient.csv\")\n",
    "measured_profiles = measured_profiles[measured_profiles['ProfileID'].isin(ids)]\n",
    "\n",
    "measured_profiles['date'] = pd.to_datetime(measured_profiles['date'])\n",
    "# measured_profiles['date'] = measured_profiles['date'].apply(lambda x: x.date())\n",
    "# measured_profiles.set_index(['ProfileID', 'date'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.intersect1d(measured_profiles.ProfileID.unique(), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = measured_profiles#X[X['ProfileID'].isin(ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the users data into different daytypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_winter = df[df['season'] == 'winter'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "# Extract Weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "df_winter['day_names'] = df_winter.date.dt.day_name()\n",
    "df_winter['daytype'] = df_winter.day_names.where(~df_winter.day_names.isin(weekdays), 'weekday')\n",
    "df_winter.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_winter_weekdays  = df_winter[df_winter['daytype'] == 'weekday'] # Create dataframe with only weekdays\n",
    "df_winter_weekdays.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_winter_weekdays =  df_winter_weekdays[df_winter_weekdays.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_winter = df[df['season'] == 'winter'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# Extract Weekdays\n",
    "weekends = ['Sunday', 'Saturday']\n",
    "df_winter['day_names'] = df_winter.date.dt.day_name()\n",
    "df_winter['daytype'] = df_winter.day_names.where(~df_winter.day_names.isin(weekends), 'weekend')\n",
    "df_winter.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_winter_weekend  = df_winter[df_winter['daytype'] == 'weekend'] # Create dataframe with only weekdays\n",
    "df_winter_weekend.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_winter_weekend = df_winter_weekend[df_winter_weekend.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_summer = df[df['season'] == 'summer'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# Extract Weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "weekends = ['Sunday', 'Saturday']\n",
    "df_summer['day_names'] = df_summer.date.dt.day_name()\n",
    "df_summer['daytype'] = df_summer.day_names.where(~df_summer.day_names.isin(weekdays), 'weekday')\n",
    "df_summer.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_summer_weekday  = df_summer[df_summer['daytype'] == 'weekday'] # Create dataframe with only weekdays\n",
    "df_summer_weekday.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_summer_weekday = df_summer_weekday[df_summer_weekday.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_summer = df[df['season'] == 'summer'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# Extract Weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "weekends = ['Sunday', 'Saturday']\n",
    "df_summer['day_names'] = df_summer.date.dt.day_name()\n",
    "df_summer['daytype'] = df_summer.day_names.where(~df_summer.day_names.isin(weekends), 'weekend')\n",
    "df_summer.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_summer_weekends  = df_summer[df_summer['daytype'] == 'weekend'] # Create dataframe with only weekdays\n",
    "df_summer_weekends.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_summer_weekends = df_summer_weekends[df_summer_weekends.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a random sample from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"all\"\n",
    "\n",
    "if sample == \"all\":\n",
    "    HWeekends_df = df_winter_weekend#.groupby(['ProfileID'])#.sample(n = 25) # Have to take a sample of 30 because winter has fewer months thus fewer weekend days\n",
    "    HWeekdays_df = df_winter_weekdays#.groupby(['ProfileID'])#.sample(n = 25)\n",
    "    LWeekends_df = df_summer_weekends#.groupby(['ProfileID'])#.sample(n = 25)\n",
    "    LWeekdays_df = df_summer_weekday#.groupby(['ProfileID'])#.sample(n = 25)\n",
    "else:\n",
    "    HWeekends_df = df_winter_weekend.groupby(['ProfileID']).sample(n = sample) # Have to take a sample of 30 because winter has fewer months thus fewer weekend days\n",
    "    HWeekdays_df = df_winter_weekdays.groupby(['ProfileID']).sample(n = sample)\n",
    "    LWeekends_df = df_summer_weekends.groupby(['ProfileID']).sample(n = sample)\n",
    "    LWeekdays_df = df_summer_weekday.groupby(['ProfileID']).sample(n = sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract gauss fit features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3a4c3c93b8477dae0cbd56defd9c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "dummy_df = pd.DataFrame(data, columns=cols)\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(HWeekdays_df,id)\n",
    "    \n",
    "    if check == False:\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 500:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian Fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy_High_season_weekdays.csv')\n",
    "    #     i = 0\n",
    "\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekdays_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProfileID</th>\n",
       "      <th>date</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ProfileID, date, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HWeekdays_df[HWeekdays_df['ProfileID'] == 1181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekdays_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e1cd92e207495b8c3bd815bc6ad2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "dummy_df = pd.DataFrame(data, columns = cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(HWeekends_df,id)\n",
    "\n",
    "    if check == False:\n",
    "        print(\"Check is FALSE\")\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 5:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'], axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy2_High_season_weekends.csv')\n",
    "    #     i = 0\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekends_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f843f9e83df44f3a260300f8178d1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "dummy_df = pd.DataFrame(data, columns=cols)\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(LWeekdays_df,id)\n",
    "    \n",
    "    if check == False:\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 500:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian Fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy_High_season_weekdays.csv')\n",
    "    #     i = 0\n",
    "\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekdays_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8560794bc274abebba3a5cd7107fed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "dummy_df = pd.DataFrame(data, columns = cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(LWeekends_df,id)\n",
    "\n",
    "   \n",
    "    if check == False:\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 500:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'], axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy_Low_season_weekends.csv')\n",
    "    #     i = 0\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekends_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the amplitudes from the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82143094b70a4dab8ad7eae4cb5ef12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in HWeekdays_df[HWeekdays_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(HWeekdays_df[HWeekdays_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekdays_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# HWeekdays_amplitudes_sample_30 = temporary.copy()\n",
    "HWeekdays_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfc1fff34d2400b8a6e4f20f345d49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in HWeekends_df[HWeekends_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(HWeekends_df[HWeekends_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekends_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# HWeekends_amplitudes_sample_30 = temporary.copy()\n",
    "HWeekends_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de5f75a8e5b4b5e9a32282e27838dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in LWeekdays_df[LWeekdays_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(LWeekdays_df[LWeekdays_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekdays_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# LWeekdays_amplitudes_sample_30 = temporary.copy()\n",
    "LWeekdays_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c75b1ca0e14b4f82da53e27609d158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in LWeekends_df[LWeekends_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(LWeekends_df[LWeekends_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekends_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# LWeekends_amplitudes_sample_30 = temporary.copy()\n",
    "LWeekends_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract standard deviation from amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(my_list):\n",
    "    #calculate population standard deviation of list \n",
    "    return (sum((x-(sum(my_list) / len(my_list)))**2 for x in my_list) / len(my_list))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def determine_standard_deviation(consumption_data, amplitudes_df):\n",
    "    daily_consumption = pd.DataFrame()\n",
    "\n",
    "    daily_consumption['Daily_Consumption'] = consumption_data.set_index([\"ProfileID\"]).sum(axis = 1)\n",
    "\n",
    "    std_deviation_df = pd.DataFrame(index = amplitudes_df.index.unique())\n",
    "\n",
    "    for id in tqdm(amplitudes_df.index.unique()):\n",
    "        try:\n",
    "            std_deviation_df.loc[id,'A1_std'] = standard_deviation(amplitudes_df.loc[id]['A1'])\n",
    "            std_deviation_df.loc[id,'A2_std'] = standard_deviation(amplitudes_df.loc[id]['A2'])\n",
    "            std_deviation_df.loc[id,'mu1_std'] = standard_deviation(amplitudes_df.loc[id]['mu1'])\n",
    "            std_deviation_df.loc[id,'mu2_std'] = standard_deviation(amplitudes_df.loc[id]['mu2'])\n",
    "            std_deviation_df.loc[id,'DC_std'] = standard_deviation(daily_consumption.loc[id]['Daily_Consumption'])\n",
    "        except TypeError:\n",
    "            print('TypeError')\n",
    "            continue\n",
    "    \n",
    "    return std_deviation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f387b415564f1893b4b7265b0fba4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39f773f40854211af07fe1a28bc2590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c65b939beb4ff0ae4b72604b38e4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b459c6f0ab4ea89daeb0b08452c388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HWeekday_std_deviation = determine_standard_deviation(HWeekdays_df, HWeekdays_amplitudes_sample_21)\n",
    "HWeekend_std_deviation = determine_standard_deviation(HWeekends_df, HWeekends_amplitudes_sample_21)\n",
    "LWeekday_std_deviation = determine_standard_deviation(LWeekdays_df, LWeekdays_amplitudes_sample_21)\n",
    "LWeekend_std_deviation = determine_standard_deviation(LWeekends_df, LWeekends_amplitudes_sample_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWeekdays_features = pd.read_csv('FitFeatures_HSeason_weekdays_sample_30.csv', index_col='ProfileID')\n",
    "# HWeekends_features = pd.read_csv('FitFeatures_HSeason_weekends_sample_30.csv', index_col='ProfileID')\n",
    "# LWeekdays_features = pd.read_csv('FitFeatures_LSeason_weekdays_sample_30.csv', index_col='ProfileID')\n",
    "# LWeekends_features = pd.read_csv('FitFeatures_LSeason_weekends_sample_30.csv', index_col='ProfileID')\n",
    "\n",
    "HWeekdays_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekdays_sample_' + str(sample) + '.csv', index_col='ProfileID')\n",
    "HWeekends_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekends_sample_' + str(sample) + '.csv', index_col='ProfileID')\n",
    "LWeekdays_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekdays_sample_' + str(sample) + '.csv', index_col='ProfileID')\n",
    "LWeekends_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekends_sample_' + str(sample) + '.csv', index_col='ProfileID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "HWeekdays_combined = pd.merge(HWeekdays_features,HWeekday_std_deviation, left_index = True, right_index = True)\n",
    "HWeekends_combined = pd.merge(HWeekends_features,HWeekend_std_deviation, left_index = True, right_index = True)\n",
    "LWeekdays_combined = pd.merge(LWeekdays_features,LWeekday_std_deviation, left_index = True, right_index = True)\n",
    "LWeekends_combined = pd.merge(LWeekends_features,LWeekend_std_deviation, left_index = True, right_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_offset</th>\n",
       "      <th>sigma1</th>\n",
       "      <th>sigma2</th>\n",
       "      <th>mu1</th>\n",
       "      <th>A1</th>\n",
       "      <th>sigma3</th>\n",
       "      <th>sigma4</th>\n",
       "      <th>mu2</th>\n",
       "      <th>A2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>6.547586</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.051282</td>\n",
       "      <td>9</td>\n",
       "      <td>10.884528</td>\n",
       "      <td>2.051282</td>\n",
       "      <td>2.380952</td>\n",
       "      <td>19</td>\n",
       "      <td>9.757444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>2.654457</td>\n",
       "      <td>1.204819</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>8</td>\n",
       "      <td>16.183497</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>1.578947</td>\n",
       "      <td>13</td>\n",
       "      <td>12.800654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>8.005045</td>\n",
       "      <td>0.628272</td>\n",
       "      <td>3.137255</td>\n",
       "      <td>8</td>\n",
       "      <td>13.075702</td>\n",
       "      <td>2.564103</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>20</td>\n",
       "      <td>9.938163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>3.016426</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>4</td>\n",
       "      <td>0.902949</td>\n",
       "      <td>2.051282</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>19</td>\n",
       "      <td>18.840391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>5.473500</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>10</td>\n",
       "      <td>5.780792</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>19</td>\n",
       "      <td>11.754974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>5.491240</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>2.258497</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>22</td>\n",
       "      <td>8.339221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>2.403670</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>5</td>\n",
       "      <td>0.688433</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>19</td>\n",
       "      <td>9.852083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>7.830904</td>\n",
       "      <td>1.481481</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>8</td>\n",
       "      <td>12.114586</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.960784</td>\n",
       "      <td>19</td>\n",
       "      <td>14.797240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4566</th>\n",
       "      <td>4.412843</td>\n",
       "      <td>1.290323</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>9</td>\n",
       "      <td>9.344487</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>1.860465</td>\n",
       "      <td>20</td>\n",
       "      <td>12.092869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>5.682490</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>10</td>\n",
       "      <td>9.251958</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>3.846154</td>\n",
       "      <td>19</td>\n",
       "      <td>18.837769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4608</th>\n",
       "      <td>2.260718</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>0.621115</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>19</td>\n",
       "      <td>9.981936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4610</th>\n",
       "      <td>4.104705</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>5</td>\n",
       "      <td>0.806247</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>1.739130</td>\n",
       "      <td>13</td>\n",
       "      <td>15.265016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>3.807099</td>\n",
       "      <td>1.904762</td>\n",
       "      <td>2.162162</td>\n",
       "      <td>8</td>\n",
       "      <td>13.798167</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>14.120766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001138</th>\n",
       "      <td>2.629923</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>9</td>\n",
       "      <td>15.484000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>18</td>\n",
       "      <td>11.299534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001148</th>\n",
       "      <td>4.361330</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>8</td>\n",
       "      <td>13.171941</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>14</td>\n",
       "      <td>6.439901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019975</th>\n",
       "      <td>7.489744</td>\n",
       "      <td>1.408451</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>8</td>\n",
       "      <td>13.785897</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>18</td>\n",
       "      <td>17.706410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12020859</th>\n",
       "      <td>5.548718</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>8</td>\n",
       "      <td>7.110256</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>19</td>\n",
       "      <td>12.893590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12021739</th>\n",
       "      <td>4.232692</td>\n",
       "      <td>1.728395</td>\n",
       "      <td>1.739130</td>\n",
       "      <td>8</td>\n",
       "      <td>5.353205</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>19</td>\n",
       "      <td>8.453846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12021804</th>\n",
       "      <td>8.335897</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>3.414634</td>\n",
       "      <td>7</td>\n",
       "      <td>7.748077</td>\n",
       "      <td>1.219512</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>17.619231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12022623</th>\n",
       "      <td>2.371154</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>2.127660</td>\n",
       "      <td>8</td>\n",
       "      <td>14.741026</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>18</td>\n",
       "      <td>12.868590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12022948</th>\n",
       "      <td>0.776282</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>1.558442</td>\n",
       "      <td>1</td>\n",
       "      <td>21.726282</td>\n",
       "      <td>6.086957</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>22</td>\n",
       "      <td>20.330128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12023147</th>\n",
       "      <td>1.326282</td>\n",
       "      <td>1.944444</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>15.886539</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>17</td>\n",
       "      <td>18.442308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           H_offset    sigma1    sigma2  mu1         A1    sigma3    sigma4  \\\n",
       "ProfileID                                                                     \n",
       "695        6.547586  2.272727  2.051282    9  10.884528  2.051282  2.380952   \n",
       "710        2.654457  1.204819  1.600000    8  16.183497  2.222222  1.578947   \n",
       "4519       8.005045  0.628272  3.137255    8  13.075702  2.564103  2.285714   \n",
       "4525       3.016426  4.000000  0.377358    4   0.902949  2.051282  2.631579   \n",
       "4529       5.473500  1.428571  1.818182   10   5.780792  2.500000  1.538462   \n",
       "4547       5.491240  1.538462  3.333333    9   2.258497  2.500000  0.930233   \n",
       "4562       2.403670  2.352941  0.377358    5   0.688433  1.600000  2.352941   \n",
       "4564       7.830904  1.481481  2.222222    8  12.114586  1.777778  1.960784   \n",
       "4566       4.412843  1.290323  3.125000    9   9.344487  1.538462  1.860465   \n",
       "4596       5.682490  1.562500  3.333333   10   9.251958  0.888889  3.846154   \n",
       "4608       2.260718  1.666667  3.333333    5   0.621115  1.818182  2.222222   \n",
       "4610       4.104705  2.500000  0.377358    5   0.806247  2.857143  1.739130   \n",
       "5404       3.807099  1.904762  2.162162    8  13.798167  0.888889  2.000000   \n",
       "1001138    2.629923  1.666667  2.727273    9  15.484000  3.000000  1.875000   \n",
       "1001148    4.361330  0.898876  2.666667    8  13.171941  1.666667  0.701754   \n",
       "12019975   7.489744  1.408451  1.666667    8  13.785897  5.000000  3.076923   \n",
       "12020859   5.548718  1.454545  1.600000    8   7.110256  1.818182  1.315789   \n",
       "12021739   4.232692  1.728395  1.739130    8   5.353205  3.076923  1.388889   \n",
       "12021804   8.335897  1.176471  3.414634    7   7.748077  1.219512  2.000000   \n",
       "12022623   2.371154  1.562500  2.127660    8  14.741026  3.076923  2.352941   \n",
       "12022948   0.776282  2.857143  1.558442    1  21.726282  6.086957  2.857143   \n",
       "12023147   1.326282  1.944444  5.000000    9  15.886539  3.157895  5.454545   \n",
       "\n",
       "           mu2         A2  \n",
       "ProfileID                  \n",
       "695         19   9.757444  \n",
       "710         13  12.800654  \n",
       "4519        20   9.938163  \n",
       "4525        19  18.840391  \n",
       "4529        19  11.754974  \n",
       "4547        22   8.339221  \n",
       "4562        19   9.852083  \n",
       "4564        19  14.797240  \n",
       "4566        20  12.092869  \n",
       "4596        19  18.837769  \n",
       "4608        19   9.981936  \n",
       "4610        13  15.265016  \n",
       "5404        17  14.120766  \n",
       "1001138     18  11.299534  \n",
       "1001148     14   6.439901  \n",
       "12019975    18  17.706410  \n",
       "12020859    19  12.893590  \n",
       "12021739    19   8.453846  \n",
       "12021804    18  17.619231  \n",
       "12022623    18  12.868590  \n",
       "12022948    22  20.330128  \n",
       "12023147    17  18.442308  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HWeekends_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Distributions to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiles_ids = combined_df.index.unique().values\n",
    "\n",
    "def determine_distributions(amplitudes_df):\n",
    "\n",
    "  cols = pd.MultiIndex.from_tuples([#(\"ProfileID\",''),\n",
    "                                  ('A1', 'Distribution'),\n",
    "                                  (\"A1\", \"chi_square\"), \n",
    "                                    (\"A1\", \"params\"), \n",
    "                                    (\"A2\", \"Distribution\"),\n",
    "                                    (\"A2\", \"chi_square\"),\n",
    "                                    (\"A2\", \"params\") \n",
    "                                    #, ('t1', 'Distribution'),\n",
    "                                    # (\"t1\", \"chi_square\"), \n",
    "                                    # (\"t1\", \"params\"), \n",
    "                                    # (\"t2\", \"Distribution\"),\n",
    "                                    # (\"t2\", \"chi_square\"),\n",
    "                                    # (\"t2\", \"params\"),\n",
    "                                  ])\n",
    "  distributions_df = pd.DataFrame(index = ids,columns = cols)\n",
    "  results = []\n",
    "  for id in tqdm(ids):\n",
    "    # Extract the best distribution fitted\n",
    "    try:\n",
    "      results1 = fit_distribution(amplitudes_df.loc[id],'A1',0.99,0.01)\n",
    "      results2 = fit_distribution(amplitudes_df.loc[id],'A2',0.99,0.01)\n",
    "      # results3 = fit_distribution(temporary.loc[id],'mu1',0.99,0.01)\n",
    "      # results4 = fit_distribution(temporary.loc[id],'mu2',0.99,0.01)\n",
    "\n",
    "      results = [results1.values[0],results1.values[1],results1.values[2], results2.values[0],results2.values[1],results2.values[2]]\n",
    "                # ,results3.values[0],results3.values[1],results3.values[2], results4.values[0],results4.values[1],results4.values[2]]\n",
    "\n",
    "      distributions_df.loc[id] = results\n",
    "    except Exception:\n",
    "      continue\n",
    "\n",
    "  return distributions_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d44a5c8d3c48328f38c256147e8c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21592b4851cd4c26b91080816a07b977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985267e5de9a4393a46dcdc8a6c382a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cc62b3e8c944a98fd1787c51092bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HWeekdays_distributions = determine_distributions(HWeekdays_amplitudes_sample_21)\n",
    "HWeekends_distributions = determine_distributions(HWeekends_amplitudes_sample_21)\n",
    "LWeekdays_distributions = determine_distributions(LWeekdays_amplitudes_sample_21)\n",
    "LWeekends_distributions = determine_distributions(LWeekends_amplitudes_sample_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the distributions for the sample of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWeekdays_distributions.to_csv(\"HWeekdays_distributions_sample_30.csv\")\n",
    "# HWeekends_distributions.to_csv(\"HWeekends_distributions_sample_30.csv\") \n",
    "# LWeekdays_distributions.to_csv(\"LWeekdays_distributions_sample_30.csv\")\n",
    "# LWeekends_distributions.to_csv(\"LWeekends_distributions_sample_30.csv\")\n",
    "\n",
    "HWeekdays_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekdays_distributions_sample_\" + str(sample) + \".csv\")\n",
    "HWeekends_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekends_distributions_sample_\" + str(sample) + \".csv\") \n",
    "LWeekdays_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekdays_distributions_sample_\" + str(sample) + \".csv\")\n",
    "LWeekends_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekends_distributions_sample_\" + str(sample) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the synthetic profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_peaks(distributions_df, combined_df):\n",
    "    \n",
    "\n",
    "    level_0 = distributions_df.columns.get_level_values(0).unique()\n",
    "    inv_data_df = pd.DataFrame()\n",
    "    temp_df_inv = pd.DataFrame()\n",
    "    for houseID in tqdm(distributions_df.index):\n",
    "        try:\n",
    "            for column in level_0:\n",
    "\n",
    "                distributions = distributions_df[column].loc[houseID]['Distribution']\n",
    "                parameters = distributions_df[column].loc[houseID]['params']\n",
    "\n",
    "                # parameters = eval(parameters)\n",
    "                loc = combined_df.loc[houseID][column] + combined_df.loc[houseID]['H_offset']\n",
    "                scale = combined_df.loc[houseID][column + '_std']  \n",
    "                # loc = parameters[-2]\n",
    "                # scale = parameters[-1]\n",
    "                size = 300\n",
    "\n",
    "                if distributions == 'invgauss':\n",
    "                    # print('invgauss')\n",
    "                    data_points = invgauss.rvs(parameters[0],loc = loc,scale = scale,size = size)\n",
    "                elif distributions == 'weibull_min':\n",
    "                    # print('weibull_min')\n",
    "                    data_points = weibull_min.rvs(parameters[0], loc = loc,scale = scale, size = size)       \n",
    "                elif distributions == 'lognorm':\n",
    "                    # print('lognorm')\n",
    "                    data_points = lognorm.rvs(parameters[0], loc = loc,scale = scale, size = size)            \n",
    "                elif distributions == 'expon':\n",
    "                    # print('expon')\n",
    "                    data_points = expon.rvs(loc = loc,scale = scale, size = size)\n",
    "                elif distributions == 'gamma':\n",
    "                    # print('gamma')\n",
    "                    data_points = gamma.rvs(parameters[0], loc = loc,scale = scale, size = size)            \n",
    "                elif distributions == 'halflogistic':\n",
    "                    # print('halflogistic')\n",
    "                    data_points = halflogistic.rvs(loc=loc, scale = scale,size=size)\n",
    "                \n",
    "                \n",
    "\n",
    "                # print(column)\n",
    "                # inverse_data_points = inverse_StandardScalar(data_points,temporary.loc[id],column,0.99,0.01)\n",
    "                temp_df_inv['ProfileID'] = houseID\n",
    "                temp_df_inv[column] = data_points#inverse_data_points\n",
    "                        \n",
    "            inv_data_df = inv_data_df.append(temp_df_inv)\n",
    "            temp_df_inv = pd.DataFrame()\n",
    "                # if column == 'A1':\n",
    "                #     data_pointA1 = data_points#inverse_data_points\n",
    "                # elif column == 'A2':\n",
    "                #     data_pointsA2 = data_points#inverse_data_points\n",
    "                \n",
    "        except KeyError:\n",
    "            print(f\"KeyError: {houseID}\")           \n",
    "            continue\n",
    "\n",
    "    inv_data_df = inv_data_df.dropna()\n",
    "    inv_data_df.set_index(['ProfileID'], inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "    return inv_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd2ba57dec94cdbae298d62a85b89c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c8f4ddf7784a0ca5b09b83e10a8428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cbbcb9b39949eda3738670b27ee5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39caf2e927c434ba9fee8fe36093d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 4608\n",
      "KeyError: 4610\n",
      "KeyError: 12022948\n"
     ]
    }
   ],
   "source": [
    "HWeekday_synthetic_peaks = generate_synthetic_peaks(HWeekdays_distributions, HWeekdays_combined)\n",
    "HWeekend_synthetic_peaks = generate_synthetic_peaks(HWeekends_distributions, HWeekends_combined)\n",
    "LWeekday_synthetic_peaks = generate_synthetic_peaks(LWeekdays_distributions, LWeekdays_combined)\n",
    "LWeekend_synthetic_peaks = generate_synthetic_peaks(LWeekends_distributions, LWeekends_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers from peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Outlier_Indices(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.95)\n",
    "    IQR = Q3 - Q1\n",
    "    trueList = ~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR)))\n",
    "    return trueList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "HWeekday_synthetic_peaks = HWeekday_synthetic_peaks[Remove_Outlier_Indices(HWeekday_synthetic_peaks['A1'])]\n",
    "HWeekday_synthetic_peaks = HWeekday_synthetic_peaks[Remove_Outlier_Indices(HWeekday_synthetic_peaks['A2'])]\n",
    "\n",
    "HWeekend_synthetic_peaks = HWeekend_synthetic_peaks[Remove_Outlier_Indices(HWeekend_synthetic_peaks['A1'])]\n",
    "HWeekend_synthetic_peaks = HWeekend_synthetic_peaks[Remove_Outlier_Indices(HWeekend_synthetic_peaks['A2'])]\n",
    "\n",
    "LWeekday_synthetic_peaks = LWeekday_synthetic_peaks[Remove_Outlier_Indices(LWeekday_synthetic_peaks['A1'])]\n",
    "LWeekday_synthetic_peaks = LWeekday_synthetic_peaks[Remove_Outlier_Indices(LWeekday_synthetic_peaks['A2'])]\n",
    "\n",
    "LWeekend_synthetic_peaks = LWeekend_synthetic_peaks[Remove_Outlier_Indices(LWeekend_synthetic_peaks['A1'])]\n",
    "LWeekend_synthetic_peaks = LWeekend_synthetic_peaks[Remove_Outlier_Indices(LWeekend_synthetic_peaks['A2'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the synthetic profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_profiles(distributions_df, features_df, synthetic_peaks): \n",
    "    synthetic_df = pd.DataFrame()\n",
    "    for id in tqdm(distributions_df.index.unique()):\n",
    "        try:\n",
    "            houseID = id\n",
    "\n",
    "            H_offset = features_df.loc[houseID]['H_offset']\n",
    "            # H_offset = 0.0\n",
    "            mu1 = features_df.loc[houseID]['mu1']\n",
    "            mu2 = features_df.loc[houseID]['mu2']\n",
    "\n",
    "            sigma1 = features_df.loc[houseID]['sigma1']\n",
    "            sigma2 = features_df.loc[houseID]['sigma2']\n",
    "            sigma3 = features_df.loc[houseID]['sigma3']\n",
    "            sigma4 = features_df.loc[houseID]['sigma4']\n",
    "\n",
    "            A1 = synthetic_peaks.loc[houseID]['A1']\n",
    "            A2 = synthetic_peaks.loc[houseID]['A2']\n",
    "            A1 = pd.DataFrame(A1)\n",
    "            A2 = pd.DataFrame(A2)\n",
    "            A1 = A1 - H_offset\n",
    "            A2 = A2 - H_offset\n",
    "\n",
    "            A1.reset_index(inplace=True)\n",
    "            A2.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "            synth = generate_synth_profiles2(houseID,A1,A2,mu1,mu2,H_offset,sigma1,sigma2, sigma3,sigma4)\n",
    "\n",
    "            # temp = pd.DataFrame(synth)\n",
    "            # temp = temp.T\n",
    "            synthetic_df = synthetic_df.append(synth)\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: {id}\")\n",
    "            \n",
    "    return synthetic_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51c5f47ea4248da80e9955837db1b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a0cb4fed6a41919f33faac9cdaad8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 695\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177526abadbf41cc8ed63f3b89cf1379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337e129b2d064b95a3712378d7b62d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 4608\n",
      "KeyError: 4610\n",
      "KeyError: 12022948\n"
     ]
    }
   ],
   "source": [
    "HWeekday_synthetic_profiles = create_synthetic_profiles(HWeekdays_distributions, HWeekdays_features, HWeekday_synthetic_peaks)\n",
    "HWeekend_synthetic_profiles = create_synthetic_profiles(HWeekends_distributions, HWeekends_features, HWeekend_synthetic_peaks)\n",
    "LWeekday_synthetic_profiles = create_synthetic_profiles(LWeekdays_distributions, LWeekdays_features, LWeekday_synthetic_peaks)\n",
    "LWeekend_synthetic_profiles = create_synthetic_profiles(LWeekends_distributions, LWeekends_features, LWeekend_synthetic_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the synthetic sample profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWeekday_synthetic_profiles.to_csv(\"HWeekday_synthetic_profiles_sample_30.csv\")\n",
    "# HWeekend_synthetic_profiles.to_csv(\"HWeekend_synthetic_profiles_sample_30.csv\")\n",
    "# LWeekday_synthetic_profiles.to_csv(\"LWeekday_synthetic_profiles_sample_30.csv\")\n",
    "# LWeekend_synthetic_profiles.to_csv(\"LWeekend_synthetic_profiles_sample_30.csv\")\n",
    "\n",
    "HWeekday_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekday_synthetic_profiles_sample_\" + str(sample) + \".csv\")\n",
    "HWeekend_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekend_synthetic_profiles_sample_\" + str(sample) + \".csv\")\n",
    "LWeekday_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekday_synthetic_profiles_sample_\" + str(sample) + \".csv\")\n",
    "LWeekend_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekend_synthetic_profiles_sample_\" + str(sample) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation with sample n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_profiles = pd.read_csv(\"Measured_Profiles_Missing_days_replaced_sorted_lenient.csv\")\n",
    "measured_profiles = measured_profiles[measured_profiles['ProfileID'].isin(ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileID</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">695</th>\n",
       "      <th>1996-01-01</th>\n",
       "      <td>0.985750</td>\n",
       "      <td>4.109417</td>\n",
       "      <td>1.074250</td>\n",
       "      <td>1.419250</td>\n",
       "      <td>3.643583</td>\n",
       "      <td>10.296583</td>\n",
       "      <td>10.082417</td>\n",
       "      <td>9.147667</td>\n",
       "      <td>10.835666</td>\n",
       "      <td>2.116750</td>\n",
       "      <td>...</td>\n",
       "      <td>11.612750</td>\n",
       "      <td>8.668333</td>\n",
       "      <td>3.388667</td>\n",
       "      <td>7.225583</td>\n",
       "      <td>8.197500</td>\n",
       "      <td>5.243833</td>\n",
       "      <td>4.677750</td>\n",
       "      <td>2.391833</td>\n",
       "      <td>2.068917</td>\n",
       "      <td>1.648250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-02</th>\n",
       "      <td>1.907667</td>\n",
       "      <td>1.904167</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>0.745917</td>\n",
       "      <td>2.515500</td>\n",
       "      <td>8.386333</td>\n",
       "      <td>9.517416</td>\n",
       "      <td>14.535250</td>\n",
       "      <td>15.029833</td>\n",
       "      <td>7.886000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.141500</td>\n",
       "      <td>14.060333</td>\n",
       "      <td>14.050333</td>\n",
       "      <td>5.163000</td>\n",
       "      <td>14.639417</td>\n",
       "      <td>11.439667</td>\n",
       "      <td>14.850416</td>\n",
       "      <td>14.425333</td>\n",
       "      <td>7.889250</td>\n",
       "      <td>1.879750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-03</th>\n",
       "      <td>1.217000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>1.981833</td>\n",
       "      <td>3.594417</td>\n",
       "      <td>1.455667</td>\n",
       "      <td>10.883000</td>\n",
       "      <td>15.531333</td>\n",
       "      <td>14.280333</td>\n",
       "      <td>13.538000</td>\n",
       "      <td>4.530500</td>\n",
       "      <td>...</td>\n",
       "      <td>10.132667</td>\n",
       "      <td>7.960417</td>\n",
       "      <td>5.208333</td>\n",
       "      <td>11.852750</td>\n",
       "      <td>13.416583</td>\n",
       "      <td>5.832083</td>\n",
       "      <td>9.138500</td>\n",
       "      <td>11.399667</td>\n",
       "      <td>2.311500</td>\n",
       "      <td>2.070083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>1.403833</td>\n",
       "      <td>1.487583</td>\n",
       "      <td>1.363167</td>\n",
       "      <td>2.727000</td>\n",
       "      <td>2.300917</td>\n",
       "      <td>10.251583</td>\n",
       "      <td>9.054167</td>\n",
       "      <td>6.212333</td>\n",
       "      <td>3.919417</td>\n",
       "      <td>4.546083</td>\n",
       "      <td>...</td>\n",
       "      <td>9.270917</td>\n",
       "      <td>10.222667</td>\n",
       "      <td>14.477667</td>\n",
       "      <td>11.548417</td>\n",
       "      <td>10.390000</td>\n",
       "      <td>10.334333</td>\n",
       "      <td>9.304167</td>\n",
       "      <td>11.652083</td>\n",
       "      <td>2.681083</td>\n",
       "      <td>2.354083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>1.239750</td>\n",
       "      <td>2.198667</td>\n",
       "      <td>1.280500</td>\n",
       "      <td>2.503500</td>\n",
       "      <td>2.171000</td>\n",
       "      <td>10.705250</td>\n",
       "      <td>10.645000</td>\n",
       "      <td>7.862417</td>\n",
       "      <td>6.695750</td>\n",
       "      <td>5.926417</td>\n",
       "      <td>...</td>\n",
       "      <td>11.801167</td>\n",
       "      <td>20.552417</td>\n",
       "      <td>9.668500</td>\n",
       "      <td>4.797333</td>\n",
       "      <td>5.144333</td>\n",
       "      <td>6.551500</td>\n",
       "      <td>5.032500</td>\n",
       "      <td>2.160417</td>\n",
       "      <td>2.428667</td>\n",
       "      <td>2.770167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">12023147</th>\n",
       "      <th>2012-12-27</th>\n",
       "      <td>3.516667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.566667</td>\n",
       "      <td>1.616667</td>\n",
       "      <td>4.233333</td>\n",
       "      <td>4.816667</td>\n",
       "      <td>9.433333</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>6.850000</td>\n",
       "      <td>8.866667</td>\n",
       "      <td>8.550000</td>\n",
       "      <td>15.316667</td>\n",
       "      <td>6.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-28</th>\n",
       "      <td>3.033333</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.483333</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>4.466667</td>\n",
       "      <td>8.133333</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>6.766667</td>\n",
       "      <td>...</td>\n",
       "      <td>12.583333</td>\n",
       "      <td>8.133333</td>\n",
       "      <td>6.616667</td>\n",
       "      <td>5.133333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.983333</td>\n",
       "      <td>7.016667</td>\n",
       "      <td>14.116667</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>4.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-29</th>\n",
       "      <td>4.016667</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>3.116667</td>\n",
       "      <td>6.183333</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>11.833333</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>10.516667</td>\n",
       "      <td>10.866667</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>8.483333</td>\n",
       "      <td>9.183333</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-30</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.683333</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>7.550000</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>10.033333</td>\n",
       "      <td>5.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-31</th>\n",
       "      <td>4.350000</td>\n",
       "      <td>3.366667</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>3.283333</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>4.533333</td>\n",
       "      <td>6.466667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>6.683333</td>\n",
       "      <td>8.266667</td>\n",
       "      <td>8.683333</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>5.433333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8049 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1         2         3         4  \\\n",
       "ProfileID date                                                           \n",
       "695       1996-01-01  0.985750  4.109417  1.074250  1.419250  3.643583   \n",
       "          1996-01-02  1.907667  1.904167  0.431250  0.745917  2.515500   \n",
       "          1996-01-03  1.217000  1.090000  1.981833  3.594417  1.455667   \n",
       "          1996-01-04  1.403833  1.487583  1.363167  2.727000  2.300917   \n",
       "          1996-01-05  1.239750  2.198667  1.280500  2.503500  2.171000   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "12023147  2012-12-27  3.516667  1.666667  3.666667  1.583333  1.566667   \n",
       "          2012-12-28  3.033333  1.600000  1.483333  1.533333  1.533333   \n",
       "          2012-12-29  4.016667  1.850000  1.833333  1.650000  1.700000   \n",
       "          2012-12-30  2.250000  1.700000  0.483333  0.466667  0.516667   \n",
       "          2012-12-31  4.350000  3.366667  2.750000  2.000000  3.066667   \n",
       "\n",
       "                              5          6          7          8         9  \\\n",
       "ProfileID date                                                               \n",
       "695       1996-01-01  10.296583  10.082417   9.147667  10.835666  2.116750   \n",
       "          1996-01-02   8.386333   9.517416  14.535250  15.029833  7.886000   \n",
       "          1996-01-03  10.883000  15.531333  14.280333  13.538000  4.530500   \n",
       "          1996-01-04  10.251583   9.054167   6.212333   3.919417  4.546083   \n",
       "          1996-01-05  10.705250  10.645000   7.862417   6.695750  5.926417   \n",
       "...                         ...        ...        ...        ...       ...   \n",
       "12023147  2012-12-27   1.616667   4.233333   4.816667   9.433333  7.500000   \n",
       "          2012-12-28   1.533333   4.466667   8.133333   3.833333  6.766667   \n",
       "          2012-12-29   2.266667   1.650000   3.116667   6.183333  5.300000   \n",
       "          2012-12-30   2.000000   2.550000   1.733333   8.700000  8.050000   \n",
       "          2012-12-31   3.066667   3.283333   7.500000   4.533333  6.466667   \n",
       "\n",
       "                      ...         14         15         16         17  \\\n",
       "ProfileID date        ...                                               \n",
       "695       1996-01-01  ...  11.612750   8.668333   3.388667   7.225583   \n",
       "          1996-01-02  ...  11.141500  14.060333  14.050333   5.163000   \n",
       "          1996-01-03  ...  10.132667   7.960417   5.208333  11.852750   \n",
       "          1996-01-04  ...   9.270917  10.222667  14.477667  11.548417   \n",
       "          1996-01-05  ...  11.801167  20.552417   9.668500   4.797333   \n",
       "...                   ...        ...        ...        ...        ...   \n",
       "12023147  2012-12-27  ...   3.466667   4.400000   5.850000  10.133333   \n",
       "          2012-12-28  ...  12.583333   8.133333   6.616667   5.133333   \n",
       "          2012-12-29  ...   5.950000  12.850000  11.833333   4.416667   \n",
       "          2012-12-30  ...   7.683333  10.133333  10.250000   4.916667   \n",
       "          2012-12-31  ...   5.933333   9.016667   4.683333   4.683333   \n",
       "\n",
       "                             18         19         20         21         22  \\\n",
       "ProfileID date                                                                \n",
       "695       1996-01-01   8.197500   5.243833   4.677750   2.391833   2.068917   \n",
       "          1996-01-02  14.639417  11.439667  14.850416  14.425333   7.889250   \n",
       "          1996-01-03  13.416583   5.832083   9.138500  11.399667   2.311500   \n",
       "          1996-01-04  10.390000  10.334333   9.304167  11.652083   2.681083   \n",
       "          1996-01-05   5.144333   6.551500   5.032500   2.160417   2.428667   \n",
       "...                         ...        ...        ...        ...        ...   \n",
       "12023147  2012-12-27   7.733333   6.850000   8.866667   8.550000  15.316667   \n",
       "          2012-12-28   5.000000   9.983333   7.016667  14.116667   7.800000   \n",
       "          2012-12-29  10.516667  10.866667   5.250000   8.483333   9.183333   \n",
       "          2012-12-30   8.833333   7.550000   3.933333   9.450000  10.033333   \n",
       "          2012-12-31   6.683333   8.266667   8.683333   8.833333   8.600000   \n",
       "\n",
       "                            23  \n",
       "ProfileID date                  \n",
       "695       1996-01-01  1.648250  \n",
       "          1996-01-02  1.879750  \n",
       "          1996-01-03  2.070083  \n",
       "          1996-01-04  2.354083  \n",
       "          1996-01-05  2.770167  \n",
       "...                        ...  \n",
       "12023147  2012-12-27  6.450000  \n",
       "          2012-12-28  4.283333  \n",
       "          2012-12-29  5.666667  \n",
       "          2012-12-30  5.950000  \n",
       "          2012-12-31  5.433333  \n",
       "\n",
       "[8049 rows x 24 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measured_profiles['date'] = pd.to_datetime(measured_profiles['date'])\n",
    "measured_profiles['date'] = measured_profiles['date'].apply(lambda x: x.date())\n",
    "measured_profiles.set_index(['ProfileID', 'date'], inplace = True)\n",
    "measured_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one year of synthetic profiles for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b8e50d0a5240f1bd4f46c1a3ce42f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 695\n",
      "KeyError: 4608\n",
      "KeyError: 4610\n",
      "KeyError: 12022948\n"
     ]
    }
   ],
   "source": [
    "temp_synth = pd.DataFrame(index = measured_profiles.index)\n",
    "temp_synth.reset_index(['date'], inplace = True)\n",
    "\n",
    "temp_synth['month'] = temp_synth.date.dt.month\n",
    "temp_synth['month'] = temp_synth['month'].apply(lambda x: 'HIGH' if x in [6, 7, 8] else 'LOW') \n",
    "temp_synth['day_names'] = temp_synth.date.dt.day_name()\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "temp_synth['day_names'] =  temp_synth['day_names'].apply(lambda x: 'WEEKDAY' if x in weekdays else 'WEEKEND') \n",
    "\n",
    "df_frames = []\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    try:\n",
    "        try:\n",
    "            temp1 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        LWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp1 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        LWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]), replace = True)], axis = 1)\n",
    "        try:                \n",
    "            temp2 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        LWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp2 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        LWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]), replace = True)], axis = 1)\n",
    "\n",
    "        try:\n",
    "            temp3 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        HWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp3 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        HWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]), replace = True)], axis = 1)\n",
    "                    \n",
    "        try:\n",
    "            temp4 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        HWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp4 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        HWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]), replace = True)], axis = 1)\n",
    "\n",
    "            \n",
    "        frames = [temp1, temp2, temp3, temp4]\n",
    "        result = pd.concat(frames)\n",
    "\n",
    "        df_frames.insert(len(df_frames)+1,result)\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"KeyError: {id}\")\n",
    "        continue\n",
    "\n",
    "result2 = pd.concat(df_frames)\n",
    "\n",
    "result2.reset_index(inplace = True)\n",
    "result2.set_index(['ProfileID', 'date'], inplace = True)\n",
    "result2 = result2.sort_index()\n",
    "# result2 = result2.drop(['24'], axis = 1)\n",
    "result2.drop(['month', 'day_names'], axis = 1, inplace = True)\n",
    "\n",
    "synthetic_profiles = result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_profiles.to_csv('CSV_Files/' + str(sample) + '/Synthetic_Profiles_sample_' + str(sample) + '.csv')\n",
    "# synthetic_profiles = pd.read_csv('Synthetic_Profiles_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75273f6ed8af91899ebc591bf6ae2fd0716c5db2515d7097a000123632f4e53a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
