{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script transforms the measured and synthetic data into the formar required to perform the PV calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\os.py\n",
      "c:\\Users\\Jason\\thesis_project\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.6.3.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import seaborn as sns\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "from datetime import timedelta as timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy.stats import *\n",
    "# import datetime as dt\n",
    "\n",
    "from support import *\n",
    "from features.feature_ts import genX\n",
    "from experiment.algorithms.cluster_prep import *\n",
    "from Gauss_fit_functions import extractFIT, extractToPs , gauss, straight_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the measured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping all zero rows\n"
     ]
    }
   ],
   "source": [
    "X = genX([1994,2014], drop_0 = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IDs of profiles with the necesary amount of High and Low Season data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "profileIDs = pd.read_pickle(\"ProfileIDs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-64b8476c9274>:15: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-4-64b8476c9274>:16: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\Jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:4308: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Collect only winter weekday profiles from dataset\n",
    "\n",
    "# X.reset_index(inplace = True)\n",
    "\n",
    "# df = X.copy()\n",
    "\n",
    "# # Extract Season\n",
    "# df['month'] = df.date.dt.month\n",
    "# df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "# df_summer = df[df['season'] == 'summer'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# # Extract Weekdays\n",
    "# weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "# df_summer['day_names'] = df_summer.date.dt.day_name()\n",
    "# df_summer['daytype'] = df_summer.day_names.where(~df_summer.day_names.isin(weekdays), 'weekday')\n",
    "# df_summer.drop(['day_names'], axis = 1, inplace = True)\n",
    "# df_summer_weekdays  = df_summer[df_summer['daytype'] == 'weekday'] # Create dataframe with only weekdays\n",
    "# df_summer_weekdays.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_summer_weekdays.set_index(['ProfileID','date'], inplace=True)\n",
    "\n",
    "# mean = df_summer_weekdays.groupby([\"ProfileID\"]).mean()\n",
    "\n",
    "# # Test 1: Energy between 9-5 \n",
    "# i = np.arange(9,17)\n",
    "# sum_df = pd.DataFrame(index = mean.index, columns = [\"9_to_5\", 'total'])\n",
    "# sum_df = sum_df.fillna(0)\n",
    "# for x in i:\n",
    "#     temp = mean.iloc[:,x]\n",
    "#     sum_df['9_to_5'] = sum_df['9_to_5'] + temp\n",
    "\n",
    "# sum_df['total'] =  mean.sum(axis = 1)\n",
    "\n",
    "# sum_df[\"percentage\"] = (sum_df[\"9_to_5\"]/sum_df[\"total\"]) * 100\n",
    "\n",
    "# def check_9_to_5(df):\n",
    "#     if df > 50:\n",
    "#         return 'B'\n",
    "#     else: \n",
    "#         return 'R'\n",
    "\n",
    "# sum_df['Labels'] = sum_df['percentage'].apply(check_9_to_5)\n",
    "\n",
    "# # 173 households labeled as Businesses for the 9-to-5 check\n",
    "# sum_df[sum_df['percentage'] > 50]\n",
    "\n",
    "# # Tests 2: ToP\n",
    "# top_df = pd.DataFrame()\n",
    "\n",
    "# top_df['Time'] = mean.idxmax(axis=1)\n",
    "\n",
    "# def check_top(df):\n",
    "#     if int(df) > 9 and int(df) < 17:\n",
    "#         return 'B'\n",
    "#     else:\n",
    "#         return 'R'  \n",
    "\n",
    "# top_df['Labels'] = top_df['Time'].apply(check_top)\n",
    "\n",
    "# new_df = pd.DataFrame()\n",
    "\n",
    "# new_df['ToP'], new_df['Sum'] = top_df['Labels'], sum_df['Labels']\n",
    "\n",
    "# data_classes = ['R','B']\n",
    "# d = dict(zip(data_classes, range(0,2)))\n",
    "\n",
    "# new_df['ToP'],new_df['Sum'] = new_df['ToP'].map(d, na_action='ignore'),new_df['Sum'].map(d, na_action='ignore')\n",
    "\n",
    "# new_df['Total'] = new_df.sum(axis = 1)\n",
    "\n",
    "# residential_ids = new_df[new_df['Total'] == 0].index.values # Contains the profile ids that have typical residential behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summer_weekdays.reset_index(inplace = True)\n",
    "\n",
    "# # Remove all households with less than three winter months\n",
    "# winter_months_ids = []\n",
    "\n",
    "# for i in df_summer_weekdays['ProfileID'].unique():\n",
    "#     if len(df_summer_weekdays[df_summer_weekdays['ProfileID'] == i]) >= 30:\n",
    "\n",
    "#         winter_months_ids.append(i) # Contains the profile Ids that have a complete winter months data\n",
    "\n",
    "# summer_months_ids = np.asarray(winter_months_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profileIDs = np.intersect1d(summer_months_ids, residential_ids)\n",
    "\n",
    "# cleaned_df = df_summer_weekdays[df_summer_weekdays['ProfileID'].isin(profileIDs)]\n",
    "\n",
    "# # # Load the previously identified profiles\n",
    "# # id_identified_profiles = pd.read_pickle('sorted_households_winter_weekdays.pkl') # Read pickle file\n",
    "# # res = []\n",
    "# # for key in id_identified_profiles.keys():\n",
    "# #     res.append(id_identified_profiles[key].tolist())\n",
    "\n",
    "# # flat_list = list()\n",
    "# # for sub_list in res:\n",
    "# #     flat_list += sub_list\n",
    "\n",
    "# # household_ids = flat_list\n",
    "\n",
    "# # temp_df = cleaned_df.copy()\n",
    "# # # temp_df.drop(['date'], axis = 1, inplace = True)\n",
    "# # temp_df = temp_df.loc[temp_df.ProfileID.isin(household_ids),:]\n",
    "\n",
    "# # cleaned_df = temp_df\n",
    "\n",
    "# high_season_profileIDs = pd.read_pickle('ProfileIDs.pkl') # Read pickle file\n",
    "\n",
    "# temp_ids = cleaned_df['ProfileID'].unique()\n",
    "\n",
    "# profileIDs = np.intersect1d(high_season_profileIDs, temp_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that every household has one year of data\n",
    "### if not take the first week of the months data as being representative of the months weekly usage and suplement the measured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_DF = X.loc[X.ProfileID.isin(profileIDs),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function that replaces missing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 1: Broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# transformed_df = pd.DataFrame()\n",
    "\n",
    "# for id in profileIDs:\n",
    "#     data_new =  measured_DF[measured_DF['ProfileID'] == id].copy()\n",
    "\n",
    "#     # Determine a households range of dates available\n",
    "#     start_date = data_new['date'].iloc[0]\n",
    "#     end_date = data_new['date'].iloc[-1]\n",
    "\n",
    "#     date_range = pd.date_range(start = start_date, end = end_date)\n",
    "\n",
    "#     # Get list of all the missing dates\n",
    "#     missing_dates = date_range.difference(data_new['date'])\n",
    "\n",
    "#     data_new.set_index(['date'], inplace = True)\n",
    "\n",
    "#     for missing_date in missing_dates:\n",
    "#         # Check if there is data measured one week prior\n",
    "#         try:\n",
    "#             # Get the date one week prior to missing date\n",
    "#             previous_weeks_date = missing_date - timedelta(weeks=1)\n",
    "#             data = data_new.loc[previous_weeks_date].tolist()\n",
    "#             # print('Previous Weeks Date')\n",
    "#         except KeyError:\n",
    "#             following_weeks_date = missing_date + timedelta(weeks=1)\n",
    "#             data = data_new.loc[following_weeks_date].tolist()\n",
    "        \n",
    "\n",
    "#         data_new.loc[missing_date] = data # Append list at the bottom\n",
    "#         data_new = data_new.sort_index()#.reset_index(drop = False)\n",
    "#         temp_df = data_new\n",
    "\n",
    "#     transformed_df.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the day is high or low season\n",
    "    # Check whether value is weekday or weekend\n",
    "    # If date is weekday:\n",
    "        # Try to get value from first of month same weekday\n",
    "        # Else: Try to get any weekday value from month\n",
    "        # Else: Get weekday value from a month within the same season\n",
    "    # If date is weekend:\n",
    "        # Repeat similiar process shown above\n",
    "# df = data_new.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# df['month'] = df.date.dt.month\n",
    "# df['season'] = df['month'].apply(lambda x: 'high_season' if x in [6, 7, 8] else 'low_season') \n",
    "\n",
    "# # Extract Weekdays\n",
    "# weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "# weekend = ['Saturday','Sunday']\n",
    "# df['day_names'] = df.date.dt.day_name()\n",
    "# # df['day_names'] = df.day_names.where(~df.day_names.isin(weekdays), 'WeekDay')\n",
    "# # df['day_names'] = df.day_names.where(~df.day_names.isin(weekend), 'WeekEnd') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_day = missing_date.day_name()\n",
    "# missing_month = missing_date.month\n",
    "# data = df[(df['month'] == missing_month) & (df['day_names'] == missing_day)]\n",
    "# # df = df.set_index(['date'], inplace = True)\n",
    "# df.loc[missing_date] = data.values[0] # Append list at the bottom\n",
    "# df = df.sort_index()#.reset_index(drop = False)\n",
    "# temp_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that replace missing data in measured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-8342425998b7>:6: TqdmDeprecationWarning:\n",
      "\n",
      "This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9216fafb4e148b8b80ced7f77e7fe35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temporary_df = pd.DataFrame(columns = measured_DF.columns)##measured_DF.set_index(['date'])\n",
    "temporary_df.set_index(['date'], inplace = True)\n",
    "\n",
    "High_season = [6,7,8]\n",
    "\n",
    "for id in tqdm(profileIDs):\n",
    "    data_new =  measured_DF[measured_DF['ProfileID'] == id].copy()\n",
    "\n",
    "    data_new['month'] = data_new.date.dt.month\n",
    "    data_new['season'] = data_new['month'].apply(lambda x: 'high_season' if x in [6, 7, 8] else 'low_season') \n",
    "    data_new['day_names'] = data_new.date.dt.day_name()\n",
    "\n",
    "    # Extract Weekdays\n",
    "    weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "    weekends = ['Sunday', 'Saturday']\n",
    "    data_new['day_names'] = data_new.date.dt.day_name()\n",
    "    data_new['daytype'] = data_new.day_names.apply(lambda x: 'Weekday' if x in weekdays else 'Weekend') \n",
    "\n",
    "\n",
    "    # Determine a households range of dates available\n",
    "    start_date = data_new['date'].iloc[0]\n",
    "    end_date = data_new['date'].iloc[-1]\n",
    "\n",
    "    one_year_later = start_date + relativedelta(years = 1)\n",
    "\n",
    "    if one_year_later < end_date:\n",
    "        date_range = pd.date_range(start = start_date, end = end_date)\n",
    "    else:\n",
    "        date_range = pd.date_range(start = start_date, end = one_year_later)\n",
    "\n",
    "    # Get list of all the missing dates\n",
    "    missing_dates = date_range.difference(data_new['date'])\n",
    "\n",
    "    data_new.set_index(['date'], inplace = True)\n",
    "\n",
    "    for missing_date in missing_dates:\n",
    "        missing_day = missing_date.day_name()\n",
    "        missing_month = missing_date.month\n",
    "        # Check if there is data measured one week prior\n",
    "\n",
    "        # Get the date one week prior to missing date\n",
    "        data = data_new[(data_new['month'] == missing_month) & (data_new['day_names'] == missing_day)]\n",
    "        if len(data) != 0:\n",
    "            data_new.loc[missing_date] = data.sample(n = 1).values[0] # Append list at the bottom\n",
    "            data_new = data_new.sort_index()#.reset_index(drop = False)\n",
    "            temp_df = data_new\n",
    "        else:\n",
    "            x = missing_date.month\n",
    "            if x in High_season:\n",
    "                data = data_new[(data_new['season'] == 'high_season') & (data_new['day_names'] == missing_day)]\n",
    "                if len(data) != 0:\n",
    "                    data_new.loc[missing_date] = data.sample(n = 1).values[0] # Append list at the bottom\n",
    "                    data_new = data_new.sort_index()#.reset_index(drop = False)\n",
    "                    temp_df = data_new\n",
    "                elif missing_day in weekdays:\n",
    "                    data = data_new[(data_new['season'] == 'high_season') & (data_new['daytype'] == 'Weekday')]\n",
    "                    data_new = data_new.sort_index()#.reset_index(drop = False)\n",
    "                    temp_df = data_new\n",
    "                elif missing_day in weekends:\n",
    "                    data = data_new[(data_new['season'] == 'high_season') & (data_new['daytype'] == 'Weekend')]\n",
    "                    data_new = data_new.sort_index()#.reset_index(drop = False)\n",
    "                    temp_df = data_new\n",
    "            else:\n",
    "                data = data_new[(data_new['season'] == 'low_season') & (data_new['day_names'] == missing_day)]\n",
    "                if len(data) != 0:\n",
    "                    data_new.loc[missing_date] = data.sample(n = 1).values[0] # Append list at the bottom\n",
    "                    data_new = data_new.sort_index()#.reset_index(drop = False)\n",
    "                    temp_df = data_new\n",
    "                elif missing_day in weekdays:\n",
    "                    data = data_new[(data_new['season'] == 'low_season') & (data_new['daytype'] == 'Weekday')]\n",
    "                    data_new = data_new.sort_index()#.reset_index(drop = False)\n",
    "                    temp_df = data_new\n",
    "                elif missing_day in weekends:\n",
    "                    data = data_new[(data_new['season'] == 'low_season') & (data_new['daytype'] == 'Weekend')]\n",
    "                    data_new = data_new.sort_index()#.reset_index(drop = False)\n",
    "                    temp_df = data_new\n",
    "\n",
    "        # print('Previous Weeks Date')\n",
    "\n",
    "    # temp_df.drop(['month', 'season', 'day_names'], axis = 1, inplace = True)\n",
    "    temporary_df = temporary_df.append(temp_df)\n",
    "    \n",
    "\n",
    "temporary_df.to_csv(\"Measured_Consumption_Missing_Days_filled.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_profiles = temporary_df.drop(['month','season','day_names','daytype'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_profiles.to_csv('Measured_Profiles_missing_values_replaced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_profiles.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that every user has one year of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change dates so that all users profiles starts in January and ends in December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dates(measured_profiles_1):\n",
    "\n",
    "    measured_profiles_1 = measured_profiles[measured_profiles['ProfileID'] == 1]\n",
    "    start_date = measured_profiles_1['date'].iloc[0]\n",
    "    one_year_later = start_date + relativedelta(years = 1)\n",
    "    measured_profiles_1['date'] = measured_profiles_1['date'].apply(lambda x: x.replace(year = start_date.year) if x < one_year_later else x )\n",
    "    measured_profiles_1['date'] = measured_profiles_1['date'].apply(lambda x: x if x.year == start_date.year else 'drop' )\n",
    "    measured_profiles_1 = measured_profiles_1[measured_profiles_1['date'] != 'drop']\n",
    "    measured_profiles_1 = measured_profiles_1.set_index(['date']).sort_index()\n",
    "\n",
    "    return measured_profiles_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt above code to run for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = measured_profiles.copy()\n",
    "testing_df_2 = testing_df.groupby(['ProfileID']).apply(change_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df_2.to_csv('Measured_Profiles_Missing_days_replaced_sorted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the synthetic years worth of data for each user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the gauss fit features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "FitFeatures_HWeekend = pd.read_csv(\"FitFeatures_High_Season_Weekends.csv\",index_col=0)\n",
    "FitFeatures_HWeekday = pd.read_csv(\"FitFeatures_HighSeason_weekdays.csv\", index_col=0)\n",
    "FitFeatures_LWeekend = pd.read_csv(\"FitFeatures_Low_Season_weekends.csv\", index_col=0)\n",
    "FitFeatures_LWeekday = pd.read_csv(\"FitFeatures_Low_Season_weekdays.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the ampltude distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "Distributions_HWeekday = pd.read_csv('High_Season_Weekday_Distributions.csv', header = [0,1], index_col = 0)\n",
    "Distributions_HWeekend = pd.read_csv('High_Season_Weekends_Distributions.csv', header = [0,1], index_col = 0)\n",
    "Distributions_LWeekday = pd.read_csv('Low_Season_Weekday_Distributions.csv', header = [0,1], index_col = 0)\n",
    "Distributions_LWeekend = pd.read_csv('Low_Season_Weekend_Distributions.csv', header = [0,1], index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the peak amplitdues of every household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amplitudes_HWeekday = pd.read_csv(\"High_Season_weekday_amplitudes.csv\", index_col = 0)\n",
    "Amplitudes_HWeekend = pd.read_csv(\"High_Season_weekends_amplitudes.csv\", index_col = 0)\n",
    "Amplitudes_LWeekday = pd.read_csv(\"Low_Season_weekdays_amplitudes.csv\", index_col = 0)\n",
    "Amplitudes_LWeekend = pd.read_csv(\"Low_Season_weekend_amplitudes.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the standard deviation of the measured peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(my_list):\n",
    "    #calculate population standard deviation of list \n",
    "    return (sum((x-(sum(my_list) / len(my_list)))**2 for x in my_list) / len(my_list))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amplitudes_std(temporary):\n",
    "\n",
    "    std_deviation_df = pd.DataFrame(index = temporary.index.unique())\n",
    "\n",
    "    for id in tqdm(temporary.index.unique()):\n",
    "        try:\n",
    "            std_deviation_df.loc[id,'A1_std'] = standard_deviation(temporary.loc[id]['A3'])\n",
    "            std_deviation_df.loc[id,'A2_std'] = standard_deviation(temporary.loc[id]['A2'])\n",
    "            std_deviation_df.loc[id,'mu1_std'] = standard_deviation(temporary.loc[id]['mu1'])\n",
    "            std_deviation_df.loc[id,'mu2_std'] = standard_deviation(temporary.loc[id]['mu2'])\n",
    "        except TypeError:\n",
    "            print('TypeError')\n",
    "            continue\n",
    "        \n",
    "    return std_deviation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-193-605ecf8491d3>:5: TqdmDeprecationWarning:\n",
      "\n",
      "This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0953fdbee4d4454bac5961c8a4c91a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7d426d52b1443aaa1ca06c1218ddbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-193-605ecf8491d3>:5: TqdmDeprecationWarning:\n",
      "\n",
      "This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566cdbe45c714b15b567049762498f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523fb1b36de04e92b59a7669451ef2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError\n",
      "TypeError\n",
      "TypeError\n",
      "TypeError\n"
     ]
    }
   ],
   "source": [
    "STD_HWeekday = get_amplitudes_std(Amplitudes_HWeekday)\n",
    "STD_HWeekend = get_amplitudes_std(Amplitudes_HWeekend)\n",
    "STD_LWeekday = get_amplitudes_std(Amplitudes_LWeekday)\n",
    "STD_LWeekend = get_amplitudes_std(Amplitudes_LWeekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataframe with all the fit features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combined_HWeekday = pd.merge(FitFeatures_HWeekday,STD_HWeekday, left_index = True, right_index = True)\n",
    "Combined_HWeekend = pd.merge(FitFeatures_HWeekend,STD_HWeekend, left_index = True, right_index = True)\n",
    "Combined_LWeekday = pd.merge(FitFeatures_LWeekday,STD_LWeekday, left_index = True, right_index = True)\n",
    "Combined_LWeekend = pd.merge(FitFeatures_LWeekend,STD_LWeekend, left_index = True, right_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_offset</th>\n",
       "      <th>sigma1</th>\n",
       "      <th>sigma2</th>\n",
       "      <th>mu1</th>\n",
       "      <th>A1</th>\n",
       "      <th>sigma3</th>\n",
       "      <th>sigma4</th>\n",
       "      <th>mu2</th>\n",
       "      <th>A2</th>\n",
       "      <th>A1_std</th>\n",
       "      <th>A2_std</th>\n",
       "      <th>mu1_std</th>\n",
       "      <th>mu2_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.338480</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>0.634921</td>\n",
       "      <td>7</td>\n",
       "      <td>1.839216</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>1.034483</td>\n",
       "      <td>21</td>\n",
       "      <td>2.988971</td>\n",
       "      <td>1.524691</td>\n",
       "      <td>4.144478</td>\n",
       "      <td>1.572146</td>\n",
       "      <td>2.016839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.292170</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>7</td>\n",
       "      <td>5.727844</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>1.860465</td>\n",
       "      <td>20</td>\n",
       "      <td>8.090200</td>\n",
       "      <td>5.316935</td>\n",
       "      <td>5.734053</td>\n",
       "      <td>2.135647</td>\n",
       "      <td>2.568181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.642967</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>7</td>\n",
       "      <td>12.090489</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.739130</td>\n",
       "      <td>20</td>\n",
       "      <td>13.238194</td>\n",
       "      <td>3.734773</td>\n",
       "      <td>4.208459</td>\n",
       "      <td>1.066960</td>\n",
       "      <td>2.274852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.416962</td>\n",
       "      <td>1.038961</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>7</td>\n",
       "      <td>10.967371</td>\n",
       "      <td>1.739130</td>\n",
       "      <td>2.121212</td>\n",
       "      <td>17</td>\n",
       "      <td>5.962964</td>\n",
       "      <td>2.612709</td>\n",
       "      <td>4.590361</td>\n",
       "      <td>1.149413</td>\n",
       "      <td>1.614234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.446746</td>\n",
       "      <td>1.100917</td>\n",
       "      <td>2.941176</td>\n",
       "      <td>6</td>\n",
       "      <td>3.465118</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>2.051282</td>\n",
       "      <td>17</td>\n",
       "      <td>4.772254</td>\n",
       "      <td>1.586904</td>\n",
       "      <td>1.996307</td>\n",
       "      <td>1.533401</td>\n",
       "      <td>1.876822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12029045</th>\n",
       "      <td>2.774317</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.564103</td>\n",
       "      <td>8</td>\n",
       "      <td>19.158962</td>\n",
       "      <td>3.703704</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>17</td>\n",
       "      <td>14.632787</td>\n",
       "      <td>5.093918</td>\n",
       "      <td>3.138364</td>\n",
       "      <td>0.992311</td>\n",
       "      <td>1.554008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12029049</th>\n",
       "      <td>0.229599</td>\n",
       "      <td>2.051282</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>8</td>\n",
       "      <td>0.262805</td>\n",
       "      <td>2.040816</td>\n",
       "      <td>1.944444</td>\n",
       "      <td>17</td>\n",
       "      <td>1.096630</td>\n",
       "      <td>0.838400</td>\n",
       "      <td>1.103542</td>\n",
       "      <td>2.409524</td>\n",
       "      <td>1.253057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12029058</th>\n",
       "      <td>11.227044</td>\n",
       "      <td>1.149425</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>5</td>\n",
       "      <td>11.530503</td>\n",
       "      <td>1.944444</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>18</td>\n",
       "      <td>32.465252</td>\n",
       "      <td>4.310748</td>\n",
       "      <td>7.779670</td>\n",
       "      <td>1.977265</td>\n",
       "      <td>0.557163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12029062</th>\n",
       "      <td>5.887421</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>0.683761</td>\n",
       "      <td>6</td>\n",
       "      <td>6.230818</td>\n",
       "      <td>2.278481</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>18</td>\n",
       "      <td>13.481132</td>\n",
       "      <td>2.432276</td>\n",
       "      <td>4.413852</td>\n",
       "      <td>0.838029</td>\n",
       "      <td>0.768269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12029066</th>\n",
       "      <td>16.911321</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>8</td>\n",
       "      <td>13.768868</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>18</td>\n",
       "      <td>38.110063</td>\n",
       "      <td>5.328510</td>\n",
       "      <td>8.211195</td>\n",
       "      <td>1.843465</td>\n",
       "      <td>0.388514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9869 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            H_offset    sigma1    sigma2  mu1         A1    sigma3    sigma4  \\\n",
       "ProfileID                                                                      \n",
       "1           1.338480  1.538462  0.634921    7   1.839216  2.631579  1.034483   \n",
       "3           7.292170  0.631579  1.081081    7   5.727844  1.428571  1.860465   \n",
       "5           1.642967  1.071429  1.562500    7  12.090489  2.000000  1.739130   \n",
       "6           1.416962  1.038961  0.960000    7  10.967371  1.739130  2.121212   \n",
       "10          1.446746  1.100917  2.941176    6   3.465118  1.250000  2.051282   \n",
       "...              ...       ...       ...  ...        ...       ...       ...   \n",
       "12029045    2.774317  1.750000  2.564103    8  19.158962  3.703704  2.800000   \n",
       "12029049    0.229599  2.051282  0.563380    8   0.262805  2.040816  1.944444   \n",
       "12029058   11.227044  1.149425  1.875000    5  11.530503  1.944444  2.181818   \n",
       "12029062    5.887421  1.714286  0.683761    6   6.230818  2.278481  2.400000   \n",
       "12029066   16.911321  3.750000  5.263158    8  13.768868  2.857143  2.222222   \n",
       "\n",
       "           mu2         A2    A1_std    A2_std   mu1_std   mu2_std  \n",
       "ProfileID                                                          \n",
       "1           21   2.988971  1.524691  4.144478  1.572146  2.016839  \n",
       "3           20   8.090200  5.316935  5.734053  2.135647  2.568181  \n",
       "5           20  13.238194  3.734773  4.208459  1.066960  2.274852  \n",
       "6           17   5.962964  2.612709  4.590361  1.149413  1.614234  \n",
       "10          17   4.772254  1.586904  1.996307  1.533401  1.876822  \n",
       "...        ...        ...       ...       ...       ...       ...  \n",
       "12029045    17  14.632787  5.093918  3.138364  0.992311  1.554008  \n",
       "12029049    17   1.096630  0.838400  1.103542  2.409524  1.253057  \n",
       "12029058    18  32.465252  4.310748  7.779670  1.977265  0.557163  \n",
       "12029062    18  13.481132  2.432276  4.413852  0.838029  0.768269  \n",
       "12029066    18  38.110063  5.328510  8.211195  1.843465  0.388514  \n",
       "\n",
       "[9869 rows x 13 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Combined_HWeekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "profileIDs1 = np.intersect1d(FitFeatures_HWeekend.ProfileID.values, FitFeatures_HWeekday.ProfileID.values)\n",
    "profileIDs2 = np.intersect1d(FitFeatures_LWeekend.ProfileID.values, FitFeatures_LWeekday.ProfileID.values)\n",
    "\n",
    "profileIDs = np.intersect1d(profileIDs1,profileIDs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_profiles_1 = pd.DataFrame(columns = measured_profiles_1.columns,index = measured_profiles_1.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the standard deviation of the amplitdues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  generate_Synthetic_Profiles(houseID, distributions_df, number_of_days, combined_df):\n",
    "\n",
    "    # distributions_df = Distributions_HWeekday\n",
    "    size = number_of_days\n",
    "    level_0 = ['A1','A2']\n",
    "    \n",
    "    inv_data_df = pd.DataFrame()\n",
    "    temp_df_inv = pd.DataFrame()\n",
    "    # for houseID in tqdm(distributions_df.index):\n",
    "    for column in level_0:\n",
    "\n",
    "        distributions = distributions_df[column].loc[houseID]['Distribution']\n",
    "        parameters = distributions_df[column].loc[houseID]['params']\n",
    "        print(parameters[1])\n",
    "        loc = combined_df.loc[houseID][column] + combined_df.loc[houseID]['H_offset']\n",
    "        scale = combined_df.loc[houseID][column + '_std']  \n",
    "        # loc = parameters[-2]\n",
    "        # scale = parameters[-1]\n",
    "        print(f\"loc = {loc}, scale = {scale}, parameters[0] = {parameters[0]}\")\n",
    "\n",
    "        try:\n",
    "            if distributions == 'invgauss':\n",
    "                print('invgauss')\n",
    "                data_points = invgauss.rvs(parameters[0],loc = loc,scale = scale,size = size)\n",
    "            elif distributions == 'weibull_min':\n",
    "                print('weibull_min')\n",
    "                data_points = weibull_min.rvs(parameters[0], loc = loc,scale = scale, size = size)\n",
    "            elif distributions == 'weibull_max':\n",
    "                print('weibull_max')\n",
    "                print(f'loc = {loc}, scale = {scale}')\n",
    "                data_points = weibull_max.rvs(parameters[0], loc = loc, scale = scale, size = size)\n",
    "                # data_points = weibull_max.rvs(parameters[0], parameters[1], parameters[2], size = 60)\n",
    "            elif distributions == 'beta':\n",
    "                print('beta')\n",
    "                data_points = beta.rvs(parameters[0], parameters[1], loc = loc,scale = scale, size = size)\n",
    "                # data_points = beta.rvs(parameters[0], parameters[1], parameters[2],parameters[3], size = 60)\n",
    "            elif distributions == 'norm':\n",
    "                print('norm')\n",
    "                data_points = norm.rvs(parameters[0], parameters[1], size = size)\n",
    "            elif distributions == 'triang':\n",
    "                print('triang')\n",
    "                data_points = triang.rvs(parameters[0], loc = loc,scale = scale, size = size)\n",
    "            elif distributions == 'pearson3':\n",
    "                print('pearson3')\n",
    "                data_points = pearson3.rvs(parameters[0], loc = loc,scale = scale, size = size)\n",
    "            elif distributions == 'lognorm':\n",
    "                print('lognorm')\n",
    "                data_points = lognorm.rvs(parameters[0], loc = loc,scale = scale, size = size)\n",
    "            elif distributions == 'uniform':\n",
    "                print('uniform')\n",
    "                data_points = uniform.rvs(loc = loc,scale = scale, size = size)\n",
    "            elif distributions == 'expon':\n",
    "                print('expon')\n",
    "                data_points = expon.rvs(loc = loc,scale = scale, size = size)\n",
    "            elif distributions == 'gamma':\n",
    "                print('gamma')\n",
    "                data_points = gamma.rvs(parameters[0], loc = loc,scale = scale, size = size)\n",
    "            elif distributions == 'alpha':\n",
    "                print('alpha')\n",
    "                data_points = alpha.rvs(parameters[0], loc=loc, scale = scale,size=size)\n",
    "            elif distributions == 'arcsine':\n",
    "                print('arcsine')\n",
    "                data_points = arcsine.rvs(loc=loc, scale = scale,size=size)\n",
    "            elif distributions == 'cauchy':\n",
    "                print('cauchy')\n",
    "                data_points = cauchy.rvs(loc=loc, scale = scale,size=size)\n",
    "            elif distributions == 'dweibull':\n",
    "                print('dweibull')\n",
    "                data_points = dweibull.rvs(parameters[0], loc=loc, scale = scale,size=size)\n",
    "            elif distributions == 'exponnorm':\n",
    "                print('exponnorm')\n",
    "                data_points = exponnorm.rvs(K=1.5, loc=loc, scale = scale,size=size)\n",
    "            elif distributions == 'halflogistic':\n",
    "                print('halflogistic')\n",
    "                data_points = halflogistic.rvs(loc=loc, scale = scale,size=size)\n",
    "            elif distributions == 'logistic':\n",
    "                print('logistic')\n",
    "                data_points = logistic.rvs(loc=loc, scale = scale,size=size)\n",
    "            elif distributions == 'loggamma':\n",
    "                print('loggamma')\n",
    "                data_points = loggamma.rvs(parameters[0], loc=loc, scale = scale,size=size)\n",
    "            elif distributions == 'powerlaw':\n",
    "                print('powerlaw')\n",
    "                data_points = powerlaw.rvs(parameters[0], loc=loc, scale = scale,size=size)\n",
    "            elif distributions == 'powernorm':\n",
    "                print('powernorm')\n",
    "                data_points = powernorm.rvs(parameters[0], loc=loc, scale = scale,size=size)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            continue\n",
    "        \n",
    "\n",
    "    # print(column)\n",
    "    # inverse_data_points = inverse_StandardScalar(data_points,temporary.loc[id],column,0.99,0.01)\n",
    "    temp_df_inv['ProfileID'] = houseID\n",
    "    temp_df_inv[column] = data_points#inverse_data_points\n",
    "        \n",
    "    inv_data_df = inv_data_df.append(temp_df_inv)\n",
    "    temp_df_inv = pd.DataFrame()\n",
    "        # if column == 'A1':\n",
    "        #     data_pointA1 = data_points#inverse_data_points\n",
    "        # elif column == 'A2':\n",
    "        #     data_pointsA2 = data_points#inverse_data_points\n",
    "\n",
    "    inv_data_df = inv_data_df.dropna()\n",
    "    inv_data_df.set_index(['ProfileID'], inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions_df[column].loc[houseID]['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loc = 1.8021396636493048, scale = 2.0833120338877404, parameters[0] = (\n",
      "invgauss\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'numpy.ndarray' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-238-722afb20a3f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCombined_LWeekend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtemp_synth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_Synthetic_Profiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistributions_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-237-fa197a52164e>\u001b[0m in \u001b[0;36mgenerate_Synthetic_Profiles\u001b[1;34m(houseID, distributions_df, number_of_days, combined_df)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdistributions\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'invgauss'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'invgauss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0mdata_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minvgauss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mdistributions\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'weibull_min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'weibull_min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy-1.7.0-py3.9-win-amd64.egg\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36mrvs\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m   1063\u001b[0m         \u001b[0mrndm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'random_state'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_args_rvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m         \u001b[0mcond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogical_and\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_argcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1066\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Domain error in arguments.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy-1.7.0-py3.9-win-amd64.egg\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m_argcheck\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[0mcond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m             \u001b[0mcond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogical_and\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'numpy.ndarray' and 'int'"
     ]
    }
   ],
   "source": [
    "# generate_Synthetic_Profiles(houseID, distributions_df, number_of_days, combined_df):\n",
    "# distributions_df = Distributions_HWeekday, Distributions_HWeekend, Distributions_LWeekday, Distributions_LWeekend\n",
    "# number_of_days = 1\n",
    "# combined_df = Combined_HWeekday, Combined_HWeekend, Combined_LWeekday, Combined_LWeekend\n",
    "# distributions_id = Distributions_HWeekday['A1'].loc[houseID]\n",
    "Weekdays = ['Monday','Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "Weekends = ['Saturday', 'Sunday']\n",
    "HSeason = [6,7,8]\n",
    "for id in profileIDs:\n",
    "    temp_synth = pd.DataFrame(index = testing_df_2[testing_df_2['ProfileID'] == 1].index, columns = testing_df_2.columns)\n",
    "\n",
    "    for date in temp_synth.index:\n",
    "        \n",
    "        if (date.month in HSeason) and (date.day_name() in Weekdays):\n",
    "            distributions_df = Distributions_HWeekday\n",
    "            combined_df = Combined_HWeekday\n",
    "        elif (date.month in HSeason) and (date.day_name() not in Weekdays):\n",
    "            distributions_df = Distributions_HWeekend\n",
    "            combined_df = Combined_HWeekend\n",
    "        elif (date.month not in HSeason) and (date.day_name() in Weekdays):\n",
    "            distributions_df = Distributions_LWeekday\n",
    "            combined_df = Combined_LWeekday\n",
    "        elif (date.month not in HSeason) and (date.day_name() not in Weekdays):\n",
    "            distributions_df = Distributions_LWeekend\n",
    "            combined_df = Combined_LWeekend\n",
    "    \n",
    "        temp_synth.loc[date] = generate_Synthetic_Profiles(id, distributions_df, 1, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProfileID</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1994-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.966667</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-01-02</th>\n",
       "      <td>1</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-01-03</th>\n",
       "      <td>1</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-01-04</th>\n",
       "      <td>1</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.016667</td>\n",
       "      <td>2.533333</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.366667</td>\n",
       "      <td>1.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-01-05</th>\n",
       "      <td>1</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.483333</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.366667</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-12-27</th>\n",
       "      <td>1</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-12-28</th>\n",
       "      <td>1</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-12-29</th>\n",
       "      <td>1</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-12-30</th>\n",
       "      <td>1</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994-12-31</th>\n",
       "      <td>1</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>1.916667</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ProfileID         0         1         2         3         4  \\\n",
       "date                                                                     \n",
       "1994-01-01         1  1.800000  1.800000  1.850000  1.700000  1.850000   \n",
       "1994-01-02         1  1.233333  1.400000  1.366667  1.400000  1.700000   \n",
       "1994-01-03         1  1.366667  1.400000  1.650000  1.700000  1.533333   \n",
       "1994-01-04         1  1.366667  1.400000  1.450000  1.750000  2.016667   \n",
       "1994-01-05         1  2.250000  2.483333  2.300000  2.400000  2.366667   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "1994-12-27         1  1.200000  1.200000  1.150000  1.200000  1.200000   \n",
       "1994-12-28         1  1.200000  1.200000  1.150000  1.150000  1.200000   \n",
       "1994-12-29         1  1.200000  1.200000  1.150000  1.200000  1.200000   \n",
       "1994-12-30         1  1.200000  1.200000  1.100000  1.400000  1.400000   \n",
       "1994-12-31         1  1.200000  1.200000  1.550000  1.916667  2.350000   \n",
       "\n",
       "                   5         6     7         8  ...    14        15    16  \\\n",
       "date                                            ...                         \n",
       "1994-01-01  1.966667  1.200000  1.20  1.200000  ...  1.20  1.150000  1.20   \n",
       "1994-01-02  1.433333  1.150000  1.20  1.200000  ...  1.20  1.200000  1.15   \n",
       "1994-01-03  2.050000  1.150000  1.20  1.200000  ...  1.20  1.200000  1.15   \n",
       "1994-01-04  2.533333  1.400000  1.20  1.200000  ...  1.15  1.200000  1.20   \n",
       "1994-01-05  2.400000  2.000000  1.50  1.266667  ...  1.20  1.150000  1.10   \n",
       "...              ...       ...   ...       ...  ...   ...       ...   ...   \n",
       "1994-12-27  1.150000  1.200000  1.20  1.200000  ...  1.20  1.066667  1.15   \n",
       "1994-12-28  1.466667  1.200000  1.15  1.200000  ...  1.15  1.150000  1.00   \n",
       "1994-12-29  1.150000  1.200000  1.20  1.200000  ...  1.20  1.150000  1.10   \n",
       "1994-12-30  1.700000  1.150000  1.15  1.200000  ...  1.20  1.233333  1.20   \n",
       "1994-12-31  2.066667  1.766667  1.20  1.050000  ...  1.20  1.200000  1.20   \n",
       "\n",
       "                  17    18        19    20        21        22        23  \n",
       "date                                                                      \n",
       "1994-01-01  1.200000  1.20  1.150000  1.20  1.200000  1.150000  1.516667  \n",
       "1994-01-02  1.150000  0.90  1.200000  1.20  1.150000  1.200000  1.233333  \n",
       "1994-01-03  1.200000  0.95  1.200000  1.20  1.150000  1.200000  1.266667  \n",
       "1994-01-04  1.050000  1.00  1.000000  1.20  1.150000  1.366667  1.650000  \n",
       "1994-01-05  1.150000  1.00  1.200000  1.15  1.266667  1.500000  1.400000  \n",
       "...              ...   ...       ...   ...       ...       ...       ...  \n",
       "1994-12-27  1.200000  1.05  1.050000  1.20  1.200000  1.200000  1.150000  \n",
       "1994-12-28  0.900000  0.90  1.133333  1.10  1.200000  1.150000  1.200000  \n",
       "1994-12-29  0.950000  1.00  1.000000  1.20  1.200000  1.200000  1.150000  \n",
       "1994-12-30  1.333333  1.10  1.200000  1.20  1.150000  1.200000  1.200000  \n",
       "1994-12-31  1.100000  1.15  1.150000  1.20  1.200000  1.250000  1.700000  \n",
       "\n",
       "[365 rows x 25 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measured_profiles_1\n",
    "Weekdays = ['Monday', 'Tuesday','Wednesday','Thursday','Friday']\n",
    "Weekends = ['Saturday','Sunday']\n",
    "\n",
    "\n",
    "if date.day_name in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52469ae204d1d50c448c04334a11ed7bbfc3a11760631521f4bdc0e8f20089d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
