{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\os.py\n",
      "c:\\Users\\Jason\\thesis_project\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.6.3.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import seaborn as sns\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "from support import *\n",
    "from features.feature_ts import genX\n",
    "from experiment.algorithms.cluster_prep import *\n",
    "from Gauss_fit_functions import extractFIT, extractToPs , gauss, straight_line\n",
    "\n",
    "from synthetic_profiles_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the households with one year worth of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping all zero rows\n"
     ]
    }
   ],
   "source": [
    "X = genX([1994,2014], drop_0 = True).reset_index()\n",
    "\n",
    "ids = pd.read_pickle(\"Ids_of_users_with_atleast_365days_of_data.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_profiles = pd.read_csv(\"Measured_Profiles_Missing_days_replaced_sorted_lenient.csv\")\n",
    "measured_profiles = measured_profiles[measured_profiles['ProfileID'].isin(ids)]\n",
    "\n",
    "measured_profiles['date'] = pd.to_datetime(measured_profiles['date'])\n",
    "# measured_profiles['date'] = measured_profiles['date'].apply(lambda x: x.date())\n",
    "# measured_profiles.set_index(['ProfileID', 'date'], inplace = True)\n",
    "ids = np.intersect1d(measured_profiles.ProfileID.unique(), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = measured_profiles#X[X['ProfileID'].isin(ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the users data into different daytypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_winter = df[df['season'] == 'winter'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "# Extract Weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "df_winter['day_names'] = df_winter.date.dt.day_name()\n",
    "df_winter['daytype'] = df_winter.day_names.where(~df_winter.day_names.isin(weekdays), 'weekday')\n",
    "df_winter.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_winter_weekdays  = df_winter[df_winter['daytype'] == 'weekday'] # Create dataframe with only weekdays\n",
    "df_winter_weekdays.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_winter_weekdays =  df_winter_weekdays[df_winter_weekdays.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_winter = df[df['season'] == 'winter'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# Extract Weekdays\n",
    "weekends = ['Sunday', 'Saturday']\n",
    "df_winter['day_names'] = df_winter.date.dt.day_name()\n",
    "df_winter['daytype'] = df_winter.day_names.where(~df_winter.day_names.isin(weekends), 'weekend')\n",
    "df_winter.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_winter_weekend  = df_winter[df_winter['daytype'] == 'weekend'] # Create dataframe with only weekdays\n",
    "df_winter_weekend.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_winter_weekend = df_winter_weekend[df_winter_weekend.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_summer = df[df['season'] == 'summer'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# Extract Weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "weekends = ['Sunday', 'Saturday']\n",
    "df_summer['day_names'] = df_summer.date.dt.day_name()\n",
    "df_summer['daytype'] = df_summer.day_names.where(~df_summer.day_names.isin(weekdays), 'weekday')\n",
    "df_summer.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_summer_weekday  = df_summer[df_summer['daytype'] == 'weekday'] # Create dataframe with only weekdays\n",
    "df_summer_weekday.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_summer_weekday = df_summer_weekday[df_summer_weekday.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect only winter weekday profiles from dataset\n",
    "df = X_filtered.copy()\n",
    "\n",
    "# df.reset_index(inplace = True)\n",
    "\n",
    "# Extract Season\n",
    "df['month'] = df.date.dt.month\n",
    "df['season'] = df['month'].apply(lambda x: 'winter' if x in [6, 7, 8] else 'summer') \n",
    "df_summer = df[df['season'] == 'summer'] # Create dataframe with all the winter months, excluding weekends\n",
    "\n",
    "\n",
    "# Extract Weekdays\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "weekends = ['Sunday', 'Saturday']\n",
    "df_summer['day_names'] = df_summer.date.dt.day_name()\n",
    "df_summer['daytype'] = df_summer.day_names.where(~df_summer.day_names.isin(weekends), 'weekend')\n",
    "df_summer.drop(['day_names'], axis = 1, inplace = True)\n",
    "df_summer_weekends  = df_summer[df_summer['daytype'] == 'weekend'] # Create dataframe with only weekdays\n",
    "df_summer_weekends.drop(['month', 'season','daytype'], axis = 1, inplace = True)\n",
    "# df_summer_weekends = df_summer_weekends[df_summer_weekends.ProfileID.isin(profileIDs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a random sample from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 25\n",
    "\n",
    "if sample == \"all\":\n",
    "    HWeekends_df = df_winter_weekend#.groupby(['ProfileID'])#.sample(n = 25) # Have to take a sample of 30 because winter has fewer months thus fewer weekend days\n",
    "    HWeekdays_df = df_winter_weekdays#.groupby(['ProfileID'])#.sample(n = 25)\n",
    "    LWeekends_df = df_summer_weekends#.groupby(['ProfileID'])#.sample(n = 25)\n",
    "    LWeekdays_df = df_summer_weekday#.groupby(['ProfileID'])#.sample(n = 25)\n",
    "else:\n",
    "    HWeekends_df = df_winter_weekend.groupby(['ProfileID']).sample(n = sample) # Have to take a sample of 30 because winter has fewer months thus fewer weekend days\n",
    "    HWeekdays_df = df_winter_weekdays.groupby(['ProfileID']).sample(n = sample)\n",
    "    LWeekends_df = df_summer_weekends.groupby(['ProfileID']).sample(n = sample)\n",
    "    LWeekdays_df = df_summer_weekday.groupby(['ProfileID']).sample(n = sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract gauss fit features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a4416bd9de486398e2cf4afc1eec7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "dummy_df = pd.DataFrame(data, columns=cols)\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(HWeekdays_df,id)\n",
    "    \n",
    "    if check == False:\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 500:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian Fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy_High_season_weekdays.csv')\n",
    "    #     i = 0\n",
    "\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekdays_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd60f837483430b855abfbf60032dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "Check is FALSE\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "dummy_df = pd.DataFrame(data, columns = cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(HWeekends_df,id)\n",
    "\n",
    "    if check == False:\n",
    "        print(\"Check is FALSE\")\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 5:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'], axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy2_High_season_weekends.csv')\n",
    "    #     i = 0\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekends_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf45b477827444683107d4f5a85caea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "dummy_df = pd.DataFrame(data, columns=cols)\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(LWeekdays_df,id)\n",
    "    \n",
    "    if check == False:\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 500:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian Fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy_High_season_weekdays.csv')\n",
    "    #     i = 0\n",
    "\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekdays_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5afc84c26a43febe299ab173aa95c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cols = ['ProfileID','H_offset','sigma1','sigma2','mu1','A1','sigma3','sigma4','mu2','A2']\n",
    "\n",
    "# Create dummy variables\n",
    "H_offset = 0\n",
    "sigma1 = 0\n",
    "sigma2 = 0 \n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "sigma3 = 0 \n",
    "sigma4 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "\n",
    "gauss_df = pd.DataFrame(data, columns=cols)\n",
    "dummy_df = pd.DataFrame(data, columns = cols)\n",
    "i = 0\n",
    "for id in tqdm(ids):\n",
    "    i = i + 1\n",
    "\n",
    "    H_offset,sigma1, sigma2, mu1, A1, sigma3, sigma4, mu2, A2, check = extractFIT(LWeekends_df,id)\n",
    "\n",
    "   \n",
    "    if check == False:\n",
    "        continue\n",
    "\n",
    "    data=[[id,H_offset,sigma1,sigma2, mu1, A1, sigma3, sigma4, mu2,A2]]\n",
    "    temp_df = pd.DataFrame(data, columns=cols)\n",
    "    temp_df.set_index(['ProfileID'])\n",
    "    gauss_df = gauss_df.append(temp_df)\n",
    "\n",
    "    # if i == 500:\n",
    "    #     dummy_df = gauss_df.copy()\n",
    "    #     dummy_df = dummy_df.set_index(['ProfileID'])\n",
    "    #     # Store Gaussian fit features\n",
    "    #     dummy_temp = dummy_df.copy()\n",
    "    #     dummy_temp.drop(['DROP_ROW'], axis = 0, inplace = True)\n",
    "    #     dummy_temp.to_csv('FitFeatures_dummy_Low_season_weekends.csv')\n",
    "    #     i = 0\n",
    "\n",
    "gauss_df = gauss_df.set_index(['ProfileID'])\n",
    "\n",
    "# Store Gaussian Fit features\n",
    "temp = gauss_df.copy()\n",
    "temp.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temp.to_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekends_sample_' + str(sample) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the amplitudes from the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5abd1aba92649cbb4af539a5b06bd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in HWeekdays_df[HWeekdays_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(HWeekdays_df[HWeekdays_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekdays_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# HWeekdays_amplitudes_sample_30 = temporary.copy()\n",
    "HWeekdays_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aead87b7fdae45068cc319c2cfc141c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in HWeekends_df[HWeekends_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(HWeekends_df[HWeekends_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekends_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# HWeekends_amplitudes_sample_30 = temporary.copy()\n",
    "HWeekends_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a2be9761f44343ac8dd4380b2f081c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in LWeekdays_df[LWeekdays_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(LWeekdays_df[LWeekdays_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekdays_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# LWeekdays_amplitudes_sample_30 = temporary.copy()\n",
    "LWeekdays_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Season Weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ff2741b8c14414a7598237101c6d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create cols\n",
    "cols = ['ProfileID','A1','A2','mu1','mu2']\n",
    "\n",
    "# Create dummy variables\n",
    "mu1 = 0\n",
    "A1 = 0 \n",
    "mu2 = 0\n",
    "A2 = 0\n",
    "\n",
    "data=[['DROP_ROW',A1,A2,mu1,mu2]]\n",
    "\n",
    "amplitudes_df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# profileIDs_3 = gauss_fit_features['ProfileID'].unique()\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    for index in LWeekends_df[LWeekends_df['ProfileID'] == id].index:\n",
    "        A1, A2, mu1, mu2, check = extractToPs(LWeekends_df[LWeekends_df['ProfileID'] == id].loc[index])\n",
    "        if check == False:\n",
    "            continue\n",
    "        \n",
    "        data=[[id,A1,A2,mu1,mu2]]\n",
    "        temp_df = pd.DataFrame(data, columns=cols)\n",
    "        # temp_df.set_index(['ProfileID'])\n",
    "        amplitudes_df = amplitudes_df.append(temp_df)\n",
    "    \n",
    "amplitudes_df = amplitudes_df.set_index(['ProfileID'])\n",
    "temporary = amplitudes_df.copy()\n",
    "temporary.drop(['DROP_ROW'],axis = 0, inplace = True)\n",
    "temporary.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekends_amplitudes_sample_\" + str(sample) + \".csv\")\n",
    "\n",
    "# LWeekends_amplitudes_sample_30 = temporary.copy()\n",
    "LWeekends_amplitudes_sample_21 = temporary.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract standard deviation from amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(my_list):\n",
    "    #calculate population standard deviation of list \n",
    "    return (sum((x-(sum(my_list) / len(my_list)))**2 for x in my_list) / len(my_list))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def determine_standard_deviation(consumption_data, amplitudes_df):\n",
    "    daily_consumption = pd.DataFrame()\n",
    "\n",
    "    daily_consumption['Daily_Consumption'] = consumption_data.set_index([\"ProfileID\"]).sum(axis = 1)\n",
    "\n",
    "    std_deviation_df = pd.DataFrame(index = amplitudes_df.index.unique())\n",
    "\n",
    "    for id in tqdm(amplitudes_df.index.unique()):\n",
    "        try:\n",
    "            std_deviation_df.loc[id,'A1_std'] = standard_deviation(amplitudes_df.loc[id]['A1'])\n",
    "            std_deviation_df.loc[id,'A2_std'] = standard_deviation(amplitudes_df.loc[id]['A2'])\n",
    "            std_deviation_df.loc[id,'mu1_std'] = standard_deviation(amplitudes_df.loc[id]['mu1'])\n",
    "            std_deviation_df.loc[id,'mu2_std'] = standard_deviation(amplitudes_df.loc[id]['mu2'])\n",
    "            std_deviation_df.loc[id,'DC_std'] = standard_deviation(daily_consumption.loc[id]['Daily_Consumption'])\n",
    "        except TypeError:\n",
    "            print('TypeError')\n",
    "            continue\n",
    "    \n",
    "    return std_deviation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e886d5f54364e3c9a3252e72be72055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b485c49e05c4ba0ba4d6109c69e2b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496c84740fa847b8b127900db455412b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b52d0c313a84849ba506795e5191678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HWeekday_std_deviation = determine_standard_deviation(HWeekdays_df, HWeekdays_amplitudes_sample_21)\n",
    "HWeekend_std_deviation = determine_standard_deviation(HWeekends_df, HWeekends_amplitudes_sample_21)\n",
    "LWeekday_std_deviation = determine_standard_deviation(LWeekdays_df, LWeekdays_amplitudes_sample_21)\n",
    "LWeekend_std_deviation = determine_standard_deviation(LWeekends_df, LWeekends_amplitudes_sample_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWeekdays_features = pd.read_csv('FitFeatures_HSeason_weekdays_sample_30.csv', index_col='ProfileID')\n",
    "# HWeekends_features = pd.read_csv('FitFeatures_HSeason_weekends_sample_30.csv', index_col='ProfileID')\n",
    "# LWeekdays_features = pd.read_csv('FitFeatures_LSeason_weekdays_sample_30.csv', index_col='ProfileID')\n",
    "# LWeekends_features = pd.read_csv('FitFeatures_LSeason_weekends_sample_30.csv', index_col='ProfileID')\n",
    "\n",
    "HWeekdays_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekdays_sample_' + str(sample) + '.csv', index_col='ProfileID')\n",
    "HWeekends_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_HSeason_weekends_sample_' + str(sample) + '.csv', index_col='ProfileID')\n",
    "LWeekdays_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekdays_sample_' + str(sample) + '.csv', index_col='ProfileID')\n",
    "LWeekends_features = pd.read_csv('CSV_Files/' + str(sample) + '/FitFeatures_LSeason_weekends_sample_' + str(sample) + '.csv', index_col='ProfileID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "HWeekdays_combined = pd.merge(HWeekdays_features,HWeekday_std_deviation, left_index = True, right_index = True)\n",
    "HWeekends_combined = pd.merge(HWeekends_features,HWeekend_std_deviation, left_index = True, right_index = True)\n",
    "LWeekdays_combined = pd.merge(LWeekdays_features,LWeekday_std_deviation, left_index = True, right_index = True)\n",
    "LWeekends_combined = pd.merge(LWeekends_features,LWeekend_std_deviation, left_index = True, right_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_offset</th>\n",
       "      <th>sigma1</th>\n",
       "      <th>sigma2</th>\n",
       "      <th>mu1</th>\n",
       "      <th>A1</th>\n",
       "      <th>sigma3</th>\n",
       "      <th>sigma4</th>\n",
       "      <th>mu2</th>\n",
       "      <th>A2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>6.701793</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>2.051282</td>\n",
       "      <td>9</td>\n",
       "      <td>10.957943</td>\n",
       "      <td>2.051282</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>19</td>\n",
       "      <td>10.297780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>2.389490</td>\n",
       "      <td>1.176471</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>8</td>\n",
       "      <td>16.515793</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>13</td>\n",
       "      <td>12.927710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>7.871497</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.962963</td>\n",
       "      <td>8</td>\n",
       "      <td>13.573933</td>\n",
       "      <td>1.290323</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>19</td>\n",
       "      <td>9.908137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>3.035047</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>4</td>\n",
       "      <td>0.941533</td>\n",
       "      <td>2.051282</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>19</td>\n",
       "      <td>19.123297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4529</th>\n",
       "      <td>5.564387</td>\n",
       "      <td>1.481481</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>10</td>\n",
       "      <td>5.829303</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>1.492537</td>\n",
       "      <td>19</td>\n",
       "      <td>11.843940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>5.600667</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>9</td>\n",
       "      <td>2.345763</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>22</td>\n",
       "      <td>8.620597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>2.438640</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>5</td>\n",
       "      <td>0.665703</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>2.352941</td>\n",
       "      <td>19</td>\n",
       "      <td>9.733787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>7.883623</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>8</td>\n",
       "      <td>11.829057</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>2.040816</td>\n",
       "      <td>19</td>\n",
       "      <td>15.077243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4566</th>\n",
       "      <td>4.418990</td>\n",
       "      <td>2.083333</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>10</td>\n",
       "      <td>9.337140</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>20</td>\n",
       "      <td>12.395137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>5.855650</td>\n",
       "      <td>1.587302</td>\n",
       "      <td>3.137255</td>\n",
       "      <td>10</td>\n",
       "      <td>9.417670</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>3.703704</td>\n",
       "      <td>19</td>\n",
       "      <td>19.171943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4610</th>\n",
       "      <td>4.174313</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>5</td>\n",
       "      <td>0.773357</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>13</td>\n",
       "      <td>14.730830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>3.816970</td>\n",
       "      <td>1.791045</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>8</td>\n",
       "      <td>13.456540</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>17</td>\n",
       "      <td>14.187553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001138</th>\n",
       "      <td>2.656833</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>15.546243</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>18</td>\n",
       "      <td>11.435357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001148</th>\n",
       "      <td>4.329647</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>2.758621</td>\n",
       "      <td>8</td>\n",
       "      <td>13.506857</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>14</td>\n",
       "      <td>6.347980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019975</th>\n",
       "      <td>7.680000</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>8</td>\n",
       "      <td>13.900667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.783784</td>\n",
       "      <td>17</td>\n",
       "      <td>17.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12020859</th>\n",
       "      <td>5.670667</td>\n",
       "      <td>1.632653</td>\n",
       "      <td>1.538462</td>\n",
       "      <td>8</td>\n",
       "      <td>7.093333</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>19</td>\n",
       "      <td>12.857333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12021739</th>\n",
       "      <td>4.284000</td>\n",
       "      <td>1.590909</td>\n",
       "      <td>1.702128</td>\n",
       "      <td>8</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>3.902439</td>\n",
       "      <td>1.298701</td>\n",
       "      <td>19</td>\n",
       "      <td>8.590667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12021804</th>\n",
       "      <td>8.299333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>7</td>\n",
       "      <td>7.247333</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>2.033898</td>\n",
       "      <td>18</td>\n",
       "      <td>17.734667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12022623</th>\n",
       "      <td>2.408667</td>\n",
       "      <td>1.587302</td>\n",
       "      <td>2.040816</td>\n",
       "      <td>8</td>\n",
       "      <td>14.907333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>18</td>\n",
       "      <td>12.996000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12022948</th>\n",
       "      <td>0.784667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.558442</td>\n",
       "      <td>1</td>\n",
       "      <td>21.618667</td>\n",
       "      <td>6.086957</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>22</td>\n",
       "      <td>20.138667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12023147</th>\n",
       "      <td>1.318000</td>\n",
       "      <td>1.917808</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>9</td>\n",
       "      <td>15.868000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.162162</td>\n",
       "      <td>20</td>\n",
       "      <td>18.257333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           H_offset    sigma1    sigma2  mu1         A1    sigma3    sigma4  \\\n",
       "ProfileID                                                                     \n",
       "695        6.701793  2.222222  2.051282    9  10.957943  2.051282  2.272727   \n",
       "710        2.389490  1.176471  1.666667    8  16.515793  2.222222  1.714286   \n",
       "4519       7.871497  0.666667  2.962963    8  13.573933  1.290323  3.333333   \n",
       "4525       3.035047  3.333333  0.377358    4   0.941533  2.051282  2.631579   \n",
       "4529       5.564387  1.481481  1.818182   10   5.829303  2.105263  1.492537   \n",
       "4547       5.600667  1.538462  3.076923    9   2.345763  2.500000  0.952381   \n",
       "4562       2.438640  3.076923  0.377358    5   0.665703  1.538462  2.352941   \n",
       "4564       7.883623  1.538462  2.272727    8  11.829057  1.818182  2.040816   \n",
       "4566       4.418990  2.083333  2.285714   10   9.337140  1.538462  1.818182   \n",
       "4596       5.855650  1.587302  3.137255   10   9.417670  0.851064  3.703704   \n",
       "4610       4.174313  6.666667  0.377358    5   0.773357  2.857143  1.777778   \n",
       "5404       3.816970  1.791045  2.105263    8  13.456540  0.851064  1.818182   \n",
       "1001138    2.656833  1.666667  3.000000    9  15.546243  3.000000  1.818182   \n",
       "1001148    4.329647  0.879121  2.758621    8  13.506857  2.000000  0.754717   \n",
       "12019975   7.680000  1.388889  1.666667    8  13.900667  4.000000  3.783784   \n",
       "12020859   5.670667  1.632653  1.538462    8   7.093333  1.818182  1.333333   \n",
       "12021739   4.284000  1.590909  1.702128    8   5.330000  3.902439  1.298701   \n",
       "12021804   8.299333  1.333333  3.750000    7   7.247333  1.136364  2.033898   \n",
       "12022623   2.408667  1.587302  2.040816    8  14.907333  3.000000  2.400000   \n",
       "12022948   0.784667  3.333333  1.558442    1  21.618667  6.086957  2.857143   \n",
       "12023147   1.318000  1.917808  4.444444    9  15.868000  2.857143  2.162162   \n",
       "\n",
       "           mu2         A2  \n",
       "ProfileID                  \n",
       "695         19  10.297780  \n",
       "710         13  12.927710  \n",
       "4519        19   9.908137  \n",
       "4525        19  19.123297  \n",
       "4529        19  11.843940  \n",
       "4547        22   8.620597  \n",
       "4562        19   9.733787  \n",
       "4564        19  15.077243  \n",
       "4566        20  12.395137  \n",
       "4596        19  19.171943  \n",
       "4610        13  14.730830  \n",
       "5404        17  14.187553  \n",
       "1001138     18  11.435357  \n",
       "1001148     14   6.347980  \n",
       "12019975    17  17.432000  \n",
       "12020859    19  12.857333  \n",
       "12021739    19   8.590667  \n",
       "12021804    18  17.734667  \n",
       "12022623    18  12.996000  \n",
       "12022948    22  20.138667  \n",
       "12023147    20  18.257333  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HWeekends_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Distributions to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiles_ids = combined_df.index.unique().values\n",
    "\n",
    "def determine_distributions(amplitudes_df):\n",
    "\n",
    "  cols = pd.MultiIndex.from_tuples([#(\"ProfileID\",''),\n",
    "                                  ('A1', 'Distribution'),\n",
    "                                  (\"A1\", \"chi_square\"), \n",
    "                                    (\"A1\", \"params\"), \n",
    "                                    (\"A2\", \"Distribution\"),\n",
    "                                    (\"A2\", \"chi_square\"),\n",
    "                                    (\"A2\", \"params\") \n",
    "                                    #, ('t1', 'Distribution'),\n",
    "                                    # (\"t1\", \"chi_square\"), \n",
    "                                    # (\"t1\", \"params\"), \n",
    "                                    # (\"t2\", \"Distribution\"),\n",
    "                                    # (\"t2\", \"chi_square\"),\n",
    "                                    # (\"t2\", \"params\"),\n",
    "                                  ])\n",
    "  distributions_df = pd.DataFrame(index = ids,columns = cols)\n",
    "  results = []\n",
    "  for id in tqdm(ids):\n",
    "    # Extract the best distribution fitted\n",
    "    try:\n",
    "      results1 = fit_distribution(amplitudes_df.loc[id],'A1',0.99,0.01)\n",
    "      results2 = fit_distribution(amplitudes_df.loc[id],'A2',0.99,0.01)\n",
    "      # results3 = fit_distribution(temporary.loc[id],'mu1',0.99,0.01)\n",
    "      # results4 = fit_distribution(temporary.loc[id],'mu2',0.99,0.01)\n",
    "\n",
    "      results = [results1.values[0],results1.values[1],results1.values[2], results2.values[0],results2.values[1],results2.values[2]]\n",
    "                # ,results3.values[0],results3.values[1],results3.values[2], results4.values[0],results4.values[1],results4.values[2]]\n",
    "\n",
    "      distributions_df.loc[id] = results\n",
    "    except Exception:\n",
    "      continue\n",
    "\n",
    "  return distributions_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d6b4706c6e42b6b4143ab72f06bf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d917c3470104b27a41ba93aec527458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0b78c4aa674f7cbee5b03c6100eff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4ce2b2049a4b338438e8b418c46e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HWeekdays_distributions = determine_distributions(HWeekdays_amplitudes_sample_21)\n",
    "HWeekends_distributions = determine_distributions(HWeekends_amplitudes_sample_21)\n",
    "LWeekdays_distributions = determine_distributions(LWeekdays_amplitudes_sample_21)\n",
    "LWeekends_distributions = determine_distributions(LWeekends_amplitudes_sample_21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the distributions for the sample of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWeekdays_distributions.to_csv(\"HWeekdays_distributions_sample_30.csv\")\n",
    "# HWeekends_distributions.to_csv(\"HWeekends_distributions_sample_30.csv\") \n",
    "# LWeekdays_distributions.to_csv(\"LWeekdays_distributions_sample_30.csv\")\n",
    "# LWeekends_distributions.to_csv(\"LWeekends_distributions_sample_30.csv\")\n",
    "\n",
    "HWeekdays_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekdays_distributions_sample_\" + str(sample) + \".csv\")\n",
    "HWeekends_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekends_distributions_sample_\" + str(sample) + \".csv\") \n",
    "LWeekdays_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekdays_distributions_sample_\" + str(sample) + \".csv\")\n",
    "LWeekends_distributions.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekends_distributions_sample_\" + str(sample) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the synthetic profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_peaks(distributions_df, combined_df):\n",
    "    level_0 = distributions_df.columns.get_level_values(0).unique()\n",
    "    inv_data_df = pd.DataFrame()\n",
    "    temp_df_inv = pd.DataFrame()\n",
    "    for houseID in tqdm(distributions_df.index):\n",
    "        try:\n",
    "            for column in level_0:\n",
    "\n",
    "                distributions = distributions_df[column].loc[houseID]['Distribution']\n",
    "                parameters = distributions_df[column].loc[houseID]['params']\n",
    "\n",
    "                # parameters = eval(parameters)\n",
    "                loc = combined_df.loc[houseID][column] + combined_df.loc[houseID]['H_offset']\n",
    "                scale = combined_df.loc[houseID][column + '_std']  \n",
    "                # loc = parameters[-2]\n",
    "                # scale = parameters[-1]\n",
    "                size = 300\n",
    "\n",
    "                if distributions == 'invgauss':\n",
    "                    # print('invgauss')\n",
    "                    data_points = invgauss.rvs(parameters[0],loc = loc,scale = scale,size = size)\n",
    "                elif distributions == 'weibull_min':\n",
    "                    # print('weibull_min')\n",
    "                    data_points = weibull_min.rvs(parameters[0], loc = loc,scale = scale, size = size)       \n",
    "                elif distributions == 'lognorm':\n",
    "                    # print('lognorm')\n",
    "                    data_points = lognorm.rvs(parameters[0], loc = loc,scale = scale, size = size)            \n",
    "                elif distributions == 'expon':\n",
    "                    # print('expon')\n",
    "                    data_points = expon.rvs(loc = loc,scale = scale, size = size)\n",
    "                elif distributions == 'gamma':\n",
    "                    # print('gamma')\n",
    "                    data_points = gamma.rvs(parameters[0], loc = loc,scale = scale, size = size)            \n",
    "                elif distributions == 'halflogistic':\n",
    "                    # print('halflogistic')\n",
    "                    data_points = halflogistic.rvs(loc=loc, scale = scale,size=size)\n",
    "                \n",
    "                \n",
    "\n",
    "                # print(column)\n",
    "                # inverse_data_points = inverse_StandardScalar(data_points,temporary.loc[id],column,0.99,0.01)\n",
    "                temp_df_inv['ProfileID'] = houseID\n",
    "                temp_df_inv[column] = data_points#inverse_data_points\n",
    "                \n",
    "            inv_data_df = inv_data_df.append(temp_df_inv)\n",
    "            temp_df_inv = pd.DataFrame()\n",
    "                # if column == 'A1':\n",
    "                #     data_pointA1 = data_points#inverse_data_points\n",
    "                # elif column == 'A2':\n",
    "                #     data_pointsA2 = data_points#inverse_data_points\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: {houseID}\")\n",
    "            continue\n",
    "\n",
    "    inv_data_df = inv_data_df.dropna()\n",
    "    inv_data_df.set_index(['ProfileID'], inplace = True)\n",
    "\n",
    "    return inv_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d4453b9d5044f9b4334d519120e249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e23ed771724494ba2de75e317470757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 4608\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1857cf5a39c446c19dae0cece38c1724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6502300b2f144f18a8af23f0777fe603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 4608\n",
      "KeyError: 12022948\n"
     ]
    }
   ],
   "source": [
    "HWeekday_synthetic_peaks = generate_synthetic_peaks(HWeekdays_distributions, HWeekdays_combined)\n",
    "HWeekend_synthetic_peaks = generate_synthetic_peaks(HWeekends_distributions, HWeekends_combined)\n",
    "LWeekday_synthetic_peaks = generate_synthetic_peaks(LWeekdays_distributions, LWeekdays_combined)\n",
    "LWeekend_synthetic_peaks = generate_synthetic_peaks(LWeekends_distributions, LWeekends_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers from peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Outlier_Indices(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.95)\n",
    "    IQR = Q3 - Q1\n",
    "    trueList = ~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR)))\n",
    "    return trueList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "HWeekday_synthetic_peaks = HWeekday_synthetic_peaks[Remove_Outlier_Indices(HWeekday_synthetic_peaks['A1'])]\n",
    "HWeekday_synthetic_peaks = HWeekday_synthetic_peaks[Remove_Outlier_Indices(HWeekday_synthetic_peaks['A2'])]\n",
    "\n",
    "HWeekend_synthetic_peaks = HWeekend_synthetic_peaks[Remove_Outlier_Indices(HWeekend_synthetic_peaks['A1'])]\n",
    "HWeekend_synthetic_peaks = HWeekend_synthetic_peaks[Remove_Outlier_Indices(HWeekend_synthetic_peaks['A2'])]\n",
    "\n",
    "LWeekday_synthetic_peaks = LWeekday_synthetic_peaks[Remove_Outlier_Indices(LWeekday_synthetic_peaks['A1'])]\n",
    "LWeekday_synthetic_peaks = LWeekday_synthetic_peaks[Remove_Outlier_Indices(LWeekday_synthetic_peaks['A2'])]\n",
    "\n",
    "LWeekend_synthetic_peaks = LWeekend_synthetic_peaks[Remove_Outlier_Indices(LWeekend_synthetic_peaks['A1'])]\n",
    "LWeekend_synthetic_peaks = LWeekend_synthetic_peaks[Remove_Outlier_Indices(LWeekend_synthetic_peaks['A2'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the synthetic profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_profiles(distributions_df, features_df, synthetic_peaks): \n",
    "    synthetic_df = pd.DataFrame()\n",
    "    for id in tqdm(distributions_df.index.unique()):\n",
    "        try:\n",
    "            houseID = id\n",
    "\n",
    "            H_offset = features_df.loc[houseID]['H_offset']\n",
    "            # H_offset = 0.0\n",
    "            mu1 = features_df.loc[houseID]['mu1']\n",
    "            mu2 = features_df.loc[houseID]['mu2']\n",
    "\n",
    "            sigma1 = features_df.loc[houseID]['sigma1']\n",
    "            sigma2 = features_df.loc[houseID]['sigma2']\n",
    "            sigma3 = features_df.loc[houseID]['sigma3']\n",
    "            sigma4 = features_df.loc[houseID]['sigma4']\n",
    "\n",
    "            A1 = synthetic_peaks.loc[houseID]['A1']\n",
    "            A2 = synthetic_peaks.loc[houseID]['A2']\n",
    "            A1 = pd.DataFrame(A1)\n",
    "            A2 = pd.DataFrame(A2)\n",
    "            A1 = A1 - H_offset\n",
    "            A2 = A2 - H_offset\n",
    "\n",
    "            A1.reset_index(inplace=True)\n",
    "            A2.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "            synth = generate_synth_profiles2(houseID,A1,A2,mu1,mu2,H_offset,sigma1,sigma2, sigma3,sigma4)\n",
    "\n",
    "            # temp = pd.DataFrame(synth)\n",
    "            # temp = temp.T\n",
    "            synthetic_df = synthetic_df.append(synth)\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: {id}\")\n",
    "            continue\n",
    "    return synthetic_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb6f892b98e47d68e7827bffc449f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 1001138\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72a362028d34405b3d304e9f8c3e8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 695\n",
      "KeyError: 4608\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02898bf5ccb411e9d8803c879e723d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 4610\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e195feaced45028fafe715ec53836d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 4608\n",
      "KeyError: 12022948\n"
     ]
    }
   ],
   "source": [
    "HWeekday_synthetic_profiles = create_synthetic_profiles(HWeekdays_distributions, HWeekdays_features, HWeekday_synthetic_peaks)\n",
    "HWeekend_synthetic_profiles = create_synthetic_profiles(HWeekends_distributions, HWeekends_features, HWeekend_synthetic_peaks)\n",
    "LWeekday_synthetic_profiles = create_synthetic_profiles(LWeekdays_distributions, LWeekdays_features, LWeekday_synthetic_peaks)\n",
    "LWeekend_synthetic_profiles = create_synthetic_profiles(LWeekends_distributions, LWeekends_features, LWeekend_synthetic_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the synthetic sample profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWeekday_synthetic_profiles.to_csv(\"HWeekday_synthetic_profiles_sample_30.csv\")\n",
    "# HWeekend_synthetic_profiles.to_csv(\"HWeekend_synthetic_profiles_sample_30.csv\")\n",
    "# LWeekday_synthetic_profiles.to_csv(\"LWeekday_synthetic_profiles_sample_30.csv\")\n",
    "# LWeekend_synthetic_profiles.to_csv(\"LWeekend_synthetic_profiles_sample_30.csv\")\n",
    "\n",
    "HWeekday_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekday_synthetic_profiles_sample_\" + str(sample) + \".csv\")\n",
    "HWeekend_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/HWeekend_synthetic_profiles_sample_\" + str(sample) + \".csv\")\n",
    "LWeekday_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekday_synthetic_profiles_sample_\" + str(sample) + \".csv\")\n",
    "LWeekend_synthetic_profiles.to_csv(\"CSV_Files/\" + str(sample) + \"/LWeekend_synthetic_profiles_sample_\" + str(sample) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation with sample n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_profiles = pd.read_csv(\"Measured_Profiles_Missing_days_replaced_sorted_lenient.csv\")\n",
    "measured_profiles = measured_profiles[measured_profiles['ProfileID'].isin(ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProfileID</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">695</th>\n",
       "      <th>1996-01-01</th>\n",
       "      <td>0.985750</td>\n",
       "      <td>4.109417</td>\n",
       "      <td>1.074250</td>\n",
       "      <td>1.419250</td>\n",
       "      <td>3.643583</td>\n",
       "      <td>10.296583</td>\n",
       "      <td>10.082417</td>\n",
       "      <td>9.147667</td>\n",
       "      <td>10.835666</td>\n",
       "      <td>2.116750</td>\n",
       "      <td>...</td>\n",
       "      <td>11.612750</td>\n",
       "      <td>8.668333</td>\n",
       "      <td>3.388667</td>\n",
       "      <td>7.225583</td>\n",
       "      <td>8.197500</td>\n",
       "      <td>5.243833</td>\n",
       "      <td>4.677750</td>\n",
       "      <td>2.391833</td>\n",
       "      <td>2.068917</td>\n",
       "      <td>1.648250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-02</th>\n",
       "      <td>1.907667</td>\n",
       "      <td>1.904167</td>\n",
       "      <td>0.431250</td>\n",
       "      <td>0.745917</td>\n",
       "      <td>2.515500</td>\n",
       "      <td>8.386333</td>\n",
       "      <td>9.517416</td>\n",
       "      <td>14.535250</td>\n",
       "      <td>15.029833</td>\n",
       "      <td>7.886000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.141500</td>\n",
       "      <td>14.060333</td>\n",
       "      <td>14.050333</td>\n",
       "      <td>5.163000</td>\n",
       "      <td>14.639417</td>\n",
       "      <td>11.439667</td>\n",
       "      <td>14.850416</td>\n",
       "      <td>14.425333</td>\n",
       "      <td>7.889250</td>\n",
       "      <td>1.879750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-03</th>\n",
       "      <td>1.217000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>1.981833</td>\n",
       "      <td>3.594417</td>\n",
       "      <td>1.455667</td>\n",
       "      <td>10.883000</td>\n",
       "      <td>15.531333</td>\n",
       "      <td>14.280333</td>\n",
       "      <td>13.538000</td>\n",
       "      <td>4.530500</td>\n",
       "      <td>...</td>\n",
       "      <td>10.132667</td>\n",
       "      <td>7.960417</td>\n",
       "      <td>5.208333</td>\n",
       "      <td>11.852750</td>\n",
       "      <td>13.416583</td>\n",
       "      <td>5.832083</td>\n",
       "      <td>9.138500</td>\n",
       "      <td>11.399667</td>\n",
       "      <td>2.311500</td>\n",
       "      <td>2.070083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-04</th>\n",
       "      <td>1.403833</td>\n",
       "      <td>1.487583</td>\n",
       "      <td>1.363167</td>\n",
       "      <td>2.727000</td>\n",
       "      <td>2.300917</td>\n",
       "      <td>10.251583</td>\n",
       "      <td>9.054167</td>\n",
       "      <td>6.212333</td>\n",
       "      <td>3.919417</td>\n",
       "      <td>4.546083</td>\n",
       "      <td>...</td>\n",
       "      <td>9.270917</td>\n",
       "      <td>10.222667</td>\n",
       "      <td>14.477667</td>\n",
       "      <td>11.548417</td>\n",
       "      <td>10.390000</td>\n",
       "      <td>10.334333</td>\n",
       "      <td>9.304167</td>\n",
       "      <td>11.652083</td>\n",
       "      <td>2.681083</td>\n",
       "      <td>2.354083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-01-05</th>\n",
       "      <td>1.239750</td>\n",
       "      <td>2.198667</td>\n",
       "      <td>1.280500</td>\n",
       "      <td>2.503500</td>\n",
       "      <td>2.171000</td>\n",
       "      <td>10.705250</td>\n",
       "      <td>10.645000</td>\n",
       "      <td>7.862417</td>\n",
       "      <td>6.695750</td>\n",
       "      <td>5.926417</td>\n",
       "      <td>...</td>\n",
       "      <td>11.801167</td>\n",
       "      <td>20.552417</td>\n",
       "      <td>9.668500</td>\n",
       "      <td>4.797333</td>\n",
       "      <td>5.144333</td>\n",
       "      <td>6.551500</td>\n",
       "      <td>5.032500</td>\n",
       "      <td>2.160417</td>\n",
       "      <td>2.428667</td>\n",
       "      <td>2.770167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">12023147</th>\n",
       "      <th>2012-12-27</th>\n",
       "      <td>3.516667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.566667</td>\n",
       "      <td>1.616667</td>\n",
       "      <td>4.233333</td>\n",
       "      <td>4.816667</td>\n",
       "      <td>9.433333</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>5.850000</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>6.850000</td>\n",
       "      <td>8.866667</td>\n",
       "      <td>8.550000</td>\n",
       "      <td>15.316667</td>\n",
       "      <td>6.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-28</th>\n",
       "      <td>3.033333</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.483333</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>1.533333</td>\n",
       "      <td>4.466667</td>\n",
       "      <td>8.133333</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>6.766667</td>\n",
       "      <td>...</td>\n",
       "      <td>12.583333</td>\n",
       "      <td>8.133333</td>\n",
       "      <td>6.616667</td>\n",
       "      <td>5.133333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.983333</td>\n",
       "      <td>7.016667</td>\n",
       "      <td>14.116667</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>4.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-29</th>\n",
       "      <td>4.016667</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>3.116667</td>\n",
       "      <td>6.183333</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>12.850000</td>\n",
       "      <td>11.833333</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>10.516667</td>\n",
       "      <td>10.866667</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>8.483333</td>\n",
       "      <td>9.183333</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-30</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.733333</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.683333</td>\n",
       "      <td>10.133333</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>4.916667</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>7.550000</td>\n",
       "      <td>3.933333</td>\n",
       "      <td>9.450000</td>\n",
       "      <td>10.033333</td>\n",
       "      <td>5.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-31</th>\n",
       "      <td>4.350000</td>\n",
       "      <td>3.366667</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>3.283333</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>4.533333</td>\n",
       "      <td>6.466667</td>\n",
       "      <td>...</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>6.683333</td>\n",
       "      <td>8.266667</td>\n",
       "      <td>8.683333</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>5.433333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8049 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1         2         3         4  \\\n",
       "ProfileID date                                                           \n",
       "695       1996-01-01  0.985750  4.109417  1.074250  1.419250  3.643583   \n",
       "          1996-01-02  1.907667  1.904167  0.431250  0.745917  2.515500   \n",
       "          1996-01-03  1.217000  1.090000  1.981833  3.594417  1.455667   \n",
       "          1996-01-04  1.403833  1.487583  1.363167  2.727000  2.300917   \n",
       "          1996-01-05  1.239750  2.198667  1.280500  2.503500  2.171000   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "12023147  2012-12-27  3.516667  1.666667  3.666667  1.583333  1.566667   \n",
       "          2012-12-28  3.033333  1.600000  1.483333  1.533333  1.533333   \n",
       "          2012-12-29  4.016667  1.850000  1.833333  1.650000  1.700000   \n",
       "          2012-12-30  2.250000  1.700000  0.483333  0.466667  0.516667   \n",
       "          2012-12-31  4.350000  3.366667  2.750000  2.000000  3.066667   \n",
       "\n",
       "                              5          6          7          8         9  \\\n",
       "ProfileID date                                                               \n",
       "695       1996-01-01  10.296583  10.082417   9.147667  10.835666  2.116750   \n",
       "          1996-01-02   8.386333   9.517416  14.535250  15.029833  7.886000   \n",
       "          1996-01-03  10.883000  15.531333  14.280333  13.538000  4.530500   \n",
       "          1996-01-04  10.251583   9.054167   6.212333   3.919417  4.546083   \n",
       "          1996-01-05  10.705250  10.645000   7.862417   6.695750  5.926417   \n",
       "...                         ...        ...        ...        ...       ...   \n",
       "12023147  2012-12-27   1.616667   4.233333   4.816667   9.433333  7.500000   \n",
       "          2012-12-28   1.533333   4.466667   8.133333   3.833333  6.766667   \n",
       "          2012-12-29   2.266667   1.650000   3.116667   6.183333  5.300000   \n",
       "          2012-12-30   2.000000   2.550000   1.733333   8.700000  8.050000   \n",
       "          2012-12-31   3.066667   3.283333   7.500000   4.533333  6.466667   \n",
       "\n",
       "                      ...         14         15         16         17  \\\n",
       "ProfileID date        ...                                               \n",
       "695       1996-01-01  ...  11.612750   8.668333   3.388667   7.225583   \n",
       "          1996-01-02  ...  11.141500  14.060333  14.050333   5.163000   \n",
       "          1996-01-03  ...  10.132667   7.960417   5.208333  11.852750   \n",
       "          1996-01-04  ...   9.270917  10.222667  14.477667  11.548417   \n",
       "          1996-01-05  ...  11.801167  20.552417   9.668500   4.797333   \n",
       "...                   ...        ...        ...        ...        ...   \n",
       "12023147  2012-12-27  ...   3.466667   4.400000   5.850000  10.133333   \n",
       "          2012-12-28  ...  12.583333   8.133333   6.616667   5.133333   \n",
       "          2012-12-29  ...   5.950000  12.850000  11.833333   4.416667   \n",
       "          2012-12-30  ...   7.683333  10.133333  10.250000   4.916667   \n",
       "          2012-12-31  ...   5.933333   9.016667   4.683333   4.683333   \n",
       "\n",
       "                             18         19         20         21         22  \\\n",
       "ProfileID date                                                                \n",
       "695       1996-01-01   8.197500   5.243833   4.677750   2.391833   2.068917   \n",
       "          1996-01-02  14.639417  11.439667  14.850416  14.425333   7.889250   \n",
       "          1996-01-03  13.416583   5.832083   9.138500  11.399667   2.311500   \n",
       "          1996-01-04  10.390000  10.334333   9.304167  11.652083   2.681083   \n",
       "          1996-01-05   5.144333   6.551500   5.032500   2.160417   2.428667   \n",
       "...                         ...        ...        ...        ...        ...   \n",
       "12023147  2012-12-27   7.733333   6.850000   8.866667   8.550000  15.316667   \n",
       "          2012-12-28   5.000000   9.983333   7.016667  14.116667   7.800000   \n",
       "          2012-12-29  10.516667  10.866667   5.250000   8.483333   9.183333   \n",
       "          2012-12-30   8.833333   7.550000   3.933333   9.450000  10.033333   \n",
       "          2012-12-31   6.683333   8.266667   8.683333   8.833333   8.600000   \n",
       "\n",
       "                            23  \n",
       "ProfileID date                  \n",
       "695       1996-01-01  1.648250  \n",
       "          1996-01-02  1.879750  \n",
       "          1996-01-03  2.070083  \n",
       "          1996-01-04  2.354083  \n",
       "          1996-01-05  2.770167  \n",
       "...                        ...  \n",
       "12023147  2012-12-27  6.450000  \n",
       "          2012-12-28  4.283333  \n",
       "          2012-12-29  5.666667  \n",
       "          2012-12-30  5.950000  \n",
       "          2012-12-31  5.433333  \n",
       "\n",
       "[8049 rows x 24 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measured_profiles['date'] = pd.to_datetime(measured_profiles['date'])\n",
    "measured_profiles['date'] = measured_profiles['date'].apply(lambda x: x.date())\n",
    "measured_profiles.set_index(['ProfileID', 'date'], inplace = True)\n",
    "measured_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one year of synthetic profiles for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a174383f3c2473b8d81a3f6a4055d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 695\n",
      "KeyError: 4608\n",
      "KeyError: 4610\n",
      "KeyError: 1001138\n",
      "KeyError: 12022948\n"
     ]
    }
   ],
   "source": [
    "temp_synth = pd.DataFrame(index = measured_profiles.index)\n",
    "temp_synth.reset_index(['date'], inplace = True)\n",
    "\n",
    "temp_synth['month'] = temp_synth.date.dt.month\n",
    "temp_synth['month'] = temp_synth['month'].apply(lambda x: 'HIGH' if x in [6, 7, 8] else 'LOW') \n",
    "temp_synth['day_names'] = temp_synth.date.dt.day_name()\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday']\n",
    "temp_synth['day_names'] =  temp_synth['day_names'].apply(lambda x: 'WEEKDAY' if x in weekdays else 'WEEKEND') \n",
    "\n",
    "df_frames = []\n",
    "\n",
    "for id in tqdm(ids):\n",
    "    try:\n",
    "        try:\n",
    "            temp1 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        LWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp1 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        LWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]), replace = True)], axis = 1)\n",
    "        try:                \n",
    "            temp2 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        LWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp2 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        LWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"LOW\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]), replace = True)], axis = 1)\n",
    "\n",
    "        try:\n",
    "            temp3 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        HWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp3 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')],\n",
    "                        HWeekday_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKDAY')]), replace = True)], axis = 1)\n",
    "                    \n",
    "        try:\n",
    "            temp4 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        HWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]))], axis = 1)\n",
    "        except ValueError:\n",
    "            print(\"ValueError\")\n",
    "            temp4 = pd.concat([temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')],\n",
    "                        HWeekend_synthetic_profiles.loc[id].sample(len(temp_synth.loc[id][(temp_synth.loc[id]['month'] == \"HIGH\") & (temp_synth.loc[id]['day_names'] == 'WEEKEND')]), replace = True)], axis = 1)\n",
    "\n",
    "            \n",
    "        frames = [temp1, temp2, temp3, temp4]\n",
    "        result = pd.concat(frames)\n",
    "\n",
    "        df_frames.insert(len(df_frames)+1,result)\n",
    "    except KeyError:\n",
    "        print(f\"KeyError: {id}\")\n",
    "        continue\n",
    "\n",
    "result2 = pd.concat(df_frames)\n",
    "\n",
    "result2.reset_index(inplace = True)\n",
    "result2.set_index(['ProfileID', 'date'], inplace = True)\n",
    "result2 = result2.sort_index()\n",
    "# result2 = result2.drop(['24'], axis = 1)\n",
    "result2.drop(['month', 'day_names'], axis = 1, inplace = True)\n",
    "\n",
    "synthetic_profiles = result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_profiles.to_csv('CSV_Files/' + str(sample) + '/Synthetic_Profiles_sample_' + str(sample) + '.csv')\n",
    "# synthetic_profiles = pd.read_csv('Synthetic_Profiles_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75273f6ed8af91899ebc591bf6ae2fd0716c5db2515d7097a000123632f4e53a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
